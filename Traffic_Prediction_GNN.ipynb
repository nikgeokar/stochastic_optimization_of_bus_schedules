{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2459a810-0da9-4b9d-9b0d-18479f772c0a",
   "metadata": {},
   "source": [
    "## Libraries - Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d255044-bd7b-4b8c-8193-3a2f436c7869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.6.0\n",
      "PyTorch Geometric version: 2.3.1\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from os.path import join\n",
    "from data_handler import FeatureEngineering\n",
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.nn import Linear\n",
    "from torch.nn import ReLU\n",
    "from torch.nn.init import kaiming_uniform_\n",
    "import torchmetrics\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric\n",
    "from torch_geometric_temporal.signal import StaticGraphTemporalSignal\n",
    "from torch_geometric_temporal.nn.recurrent import A3TGCN2\n",
    "import random\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"PyTorch Geometric version: {torch_geometric.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1151145-410b-44d4-83fa-53f4fa968458",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seeds(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # For CUDA devices\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seeds()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c68ae9-a944-4813-a6e4-56ddf3a913dc",
   "metadata": {},
   "source": [
    "## Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "032cf66d-9fd3-47a4-9692-a070c361fc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = join(\"io\", \"input\")\n",
    "output_path = join(\"io\", \"output\")\n",
    "experiments_path = join(\"io\", \"experiments\")\n",
    "graph_structured_np = join(output_path, \"graph_structured_np\")\n",
    "metrics_path = join(experiments_path, \"metrics\")\n",
    "plots_path = join(experiments_path, \"plots\")\n",
    "gnn_model_path = join(\"io\", \"gnn_model\")\n",
    "\n",
    "core_features = ['node_id', 'ETA_curr']\n",
    "\n",
    "calendar_features = ['sin_hour','cos_hour', 'sin_dayofweek', 'cos_dayofweek', 'sin_month', 'cos_month','sin_dayofmonth', 'cos_dayofmonth', \n",
    "                     'sin_weekofyear', 'cos_weekofyear', 'sin_quarter_hour', 'cos_quarter_hour']\n",
    "\n",
    "rolling_avg_features = ['rolling_avg_4h', 'rolling_avg_12h', 'rolling_avg_68h', 'rolling_avg_476h', 'rolling_avg_20240h']\n",
    "\n",
    "lag_features = ['lag1h', 'lag4h', 'lag476h', 'lag20240h']\n",
    "\n",
    "feature_cols = core_features + calendar_features + rolling_avg_features + lag_features\n",
    "\n",
    "edges_df = pd.read_csv(join(output_path, \"interm_network_edges.csv\"), encoding='utf-8', sep=',')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729e69de-a20c-41bf-929a-735fa88ef310",
   "metadata": {},
   "source": [
    "# Graph Representation Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e462eaf-59b4-45cd-82cd-abc810cffd4e",
   "metadata": {},
   "source": [
    "## Create Graph Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a75d21a8-cfe0-4d7a-bfcd-8bec6f0c8286",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_edges_info(df, node_id_to_index, fe):\n",
    "    edge_index = []\n",
    "    edge_weights = []\n",
    "    \n",
    "    # Iterating with index to ensure you use the correct node references\n",
    "    for i in tqdm(range(len(df))):\n",
    "        node_i = df.iloc[i]\n",
    "        node_id_i = node_id_to_index[node_i['edge_id']]\n",
    "        for j in range(i + 1, len(df)):\n",
    "            node_j = df.iloc[j]\n",
    "            node_id_j = node_id_to_index[node_j['edge_id']]\n",
    "    \n",
    "            # Check if any of the start or end node ids match\n",
    "            if any(node_i[['start_node_id', 'end_node_id']].isin(node_j[['start_node_id', 'end_node_id']])):\n",
    "                edge_index.append([node_id_i, node_id_j])\n",
    "                weight = fe.haversine(node_i['start_lat'], node_i['start_long'], node_j['start_lat'], node_j['start_long'])\n",
    "                edge_weights.append(weight)\n",
    "    \n",
    "    # Convert lists to numpy arrays for use in data structures\n",
    "    edge_index = np.array(edge_index).T  # Shape: [2, num_edges]\n",
    "    edge_weights = np.array(edge_weights)\n",
    "\n",
    "    # Normalize edge weights from 0 to 1\n",
    "    edge_weights = (edge_weights - edge_weights.min()) / (edge_weights.max() - edge_weights.min())\n",
    "    \n",
    "    return edge_index, edge_weights\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48407b5e-a7ac-4021-bdd5-e74e1a6099a2",
   "metadata": {},
   "source": [
    "## Convert Tabular Data to Numpy Based Graph Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70bbbd63-3456-4a0a-b96c-b56bcb31f3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_concat_arrays(save_dir, years):\n",
    "    # You presumably have the same node_id_to_index across all years\n",
    "    # The final shape for node dimension and features dimension must match\n",
    "    features_list = []\n",
    "    targets_list = []\n",
    "    timestamps_list = []\n",
    "\n",
    "    for year in years:\n",
    "        fpath_features = os.path.join(save_dir, f\"features_year_{year}.npy\")\n",
    "        fpath_targets = os.path.join(save_dir, f\"targets_year_{year}.npy\")\n",
    "        fpath_timestamps = os.path.join(save_dir, f\"timestamps_year_{year}.npy\")\n",
    "\n",
    "        feat_local = np.load(fpath_features)\n",
    "        targ_local = np.load(fpath_targets)\n",
    "        ts_local   = np.load(fpath_timestamps)\n",
    "\n",
    "        features_list.append(feat_local)\n",
    "        targets_list.append(targ_local)\n",
    "        timestamps_list.append(ts_local)\n",
    "\n",
    "    # Concatenate along time dimension\n",
    "    features_all = np.concatenate(features_list, axis=0)\n",
    "    targets_all  = np.concatenate(targets_list, axis=0)\n",
    "    timestamps_all = np.concatenate(timestamps_list, axis=0)\n",
    "\n",
    "    return features_all, targets_all, timestamps_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b498089-8baa-4a81-a934-6ab3ea520d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_df_to_numpy(df, feature_cols, poi_cols, save_dir=graph_structured_np, dtype=np.float32):\n",
    "    \"\"\"\n",
    "    Split the DataFrame by year, create partial arrays for each year,\n",
    "    then immediately save each partial array to disk, freeing memory.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Contains columns: 'node_id', 'timestamp', 'target', ...\n",
    "        'timestamp' should be datetime or convertible to datetime.\n",
    "    feature_cols : list\n",
    "        Features besides 'node_id', 'timestamp', 'target'.\n",
    "    poi_cols : list\n",
    "        Extra columns for points of interest distances, appended to feature_cols.\n",
    "    save_dir : str\n",
    "        Directory path in which to save arrays for each year (create if needed).\n",
    "    dtype : numpy.dtype\n",
    "        e.g. np.float32 or np.float16 to reduce memory usage.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    node_id_to_index : dict\n",
    "        The global mapping of node_id -> node index.\n",
    "    sorted_years : list\n",
    "        Sorted list of all years processed, in case you want them for reference.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    df = df.copy()\n",
    "    if not np.issubdtype(df['timestamp'].dtype, np.datetime64):\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "    # Build a global node->index mapping across all data\n",
    "    unique_nodes = df['node_id'].unique()\n",
    "    node_id_to_index = {nid: i for i, nid in enumerate(unique_nodes)}\n",
    "    num_nodes = len(unique_nodes)\n",
    "\n",
    "    # Combine feature + poi columns, excluding 'node_id'\n",
    "    feature_cols = [col for col in feature_cols if col != 'node_id']\n",
    "    all_feature_cols = feature_cols + poi_cols\n",
    "\n",
    "    # Add 'year' col to split by year\n",
    "    df['year'] = df['timestamp'].dt.year\n",
    "    sorted_years = sorted(df['year'].unique())\n",
    "\n",
    "    for year in sorted_years:\n",
    "        print(f\"\\nProcessing year={year}...\")\n",
    "\n",
    "        # Filter DataFrame for this year\n",
    "        df_year = df[df['year'] == year].copy()\n",
    "        # Sort by timestamp for stable ordering\n",
    "        df_year.sort_values(by='timestamp', inplace=True)\n",
    "\n",
    "        # Unique timestamps for this year\n",
    "        local_timestamps = df_year['timestamp'].unique()\n",
    "        local_ts_to_idx = {ts: i for i, ts in enumerate(local_timestamps)}\n",
    "        num_timesteps = len(local_timestamps)\n",
    "\n",
    "        # Prepare partial arrays\n",
    "        num_features = len(all_feature_cols)\n",
    "        features_local = np.zeros((num_timesteps, num_nodes, num_features), dtype=dtype)\n",
    "        targets_local = np.zeros((num_timesteps, num_nodes, 1), dtype=dtype)\n",
    "\n",
    "        # Fill arrays\n",
    "        for _, row in tqdm(df_year.iterrows(), total=df_year.shape[0], desc=f\"Year={year}\"):\n",
    "            node_idx = node_id_to_index[row['node_id']]\n",
    "            time_idx = local_ts_to_idx[row['timestamp']]\n",
    "\n",
    "            features_local[time_idx, node_idx, :] = row[all_feature_cols].values\n",
    "            targets_local[time_idx, node_idx, 0] = row['target']\n",
    "\n",
    "        # Save to disk\n",
    "        # e.g. np.save for each, or np.savez to store them together\n",
    "        np.save(os.path.join(save_dir, f\"features_year_{year}.npy\"), features_local)\n",
    "        np.save(os.path.join(save_dir, f\"targets_year_{year}.npy\"), targets_local)\n",
    "        np.save(os.path.join(save_dir, f\"timestamps_year_{year}.npy\"), local_timestamps)\n",
    "\n",
    "        # Free memory\n",
    "        del features_local, targets_local, df_year\n",
    "        gc.collect()\n",
    "\n",
    "    # Optionally drop the added 'year' column\n",
    "    df.drop(columns=['year'], inplace=True, errors='ignore')\n",
    "\n",
    "    features_all, targets_all, timestamps_all = load_and_concat_arrays(save_dir, sorted_years)\n",
    "\n",
    "    print(\"\\nFinished saving partial arrays year by year.\")\n",
    "    return features_all, targets_all, node_id_to_index, timestamps_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "beb5ca8c-654b-4d5e-9504-8950faf8aab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_future_targets(targets, num_periods):\n",
    "    # Assuming targets is of shape (2039, 225, 1)\n",
    "    num_timesteps, num_nodes, _ = targets.shape\n",
    "    # Check if there's enough data to form the future targets\n",
    "    if num_timesteps < num_periods:\n",
    "        raise ValueError(\"Not enough timesteps to form future targets.\")\n",
    "    \n",
    "    # Initialize the future targets array\n",
    "    # Shape will be (1972, 225, 68) since we need 68 future data points for each of the starting 1972 points\n",
    "    future_targets = np.zeros((num_timesteps - num_periods + 1, num_nodes, num_periods))\n",
    "\n",
    "    # Populate the future targets array\n",
    "    for i in range(future_targets.shape[0]):\n",
    "        for n in range(num_nodes):\n",
    "            future_targets[i, n, :] = targets[i:i + num_periods, n, 0]  # reshape or squeeze as necessary\n",
    "\n",
    "    return future_targets\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac53966-6e96-4123-9225-7b24b833137d",
   "metadata": {},
   "source": [
    "## Create Torch Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c691760b-da7c-4b61-8c67-2a1a8afc60f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def static_graph_data_loader(features, targets, edge_index, edge_weights, set_nanme, batch_size, num_periods):\n",
    "    # Convert features to a PyTorch tensor\n",
    "    features_tensor = torch.from_numpy(features).type(torch.FloatTensor)  # Initial Dims = (2039, 225, 38)\n",
    "\n",
    "    # Create a time window for each feature at each node\n",
    "    # Unfold along the first dimension (time) to create windows\n",
    "    features_tensor = features_tensor.unfold(0, num_periods, 1)  # Now dims = (1972, 225, 38, 68)\n",
    "\n",
    "    # No need to permute since it is already in the desired order: (timesteps, nodes, features, periods)\n",
    "    \n",
    "    # Prepare targets accordingly (assuming prepare_future_targets function adjusts them appropriately)\n",
    "    prepared_targets = prepare_future_targets(targets, num_periods)\n",
    "    targets_tensor = torch.from_numpy(prepared_targets).type(torch.FloatTensor)\n",
    "\n",
    "    # Check dimensions\n",
    "    print(f\"{set_nanme} Features tensor shape: {features_tensor.shape}\")\n",
    "    print(f\"{set_nanme} Targets tensor shape: {targets_tensor.shape}\")\n",
    "\n",
    "    # Convert edge data to tensors\n",
    "    edge_index_tensor = torch.from_numpy(edge_index).type(torch.LongTensor) # Dims = (2, 292)\n",
    "    edge_weights_tensor = torch.from_numpy(edge_weights).type(torch.FloatTensor) # Dims = (292)\n",
    "\n",
    "    # Ensure that both tensors are aligned in the first dimension\n",
    "    assert features_tensor.shape[0] == targets_tensor.shape[0], \"Feature and target tensor size mismatch\"\n",
    "\n",
    "    # Create dataset and loader\n",
    "    dataset = TensorDataset(features_tensor, targets_tensor)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "    return dataloader, edge_index_tensor, edge_weights_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2618ccd1-d4ee-4bf2-bbcd-5622a78825e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph_data(use_validation, training_end_date, validation_end_date, test_end_date, output_path, pre_calculated, save, feature_cols, batch_size, periods, final_training=False):\n",
    "\n",
    "    if use_validation:\n",
    "        train_name = 'train'\n",
    "        eval_name = 'validation'\n",
    "    else:\n",
    "        if final_training:\n",
    "            train_name = 'train_final'\n",
    "            eval_name = 'test_final'\n",
    "        else:\n",
    "            train_name = 'train_full'\n",
    "            eval_name = 'test'\n",
    "    \n",
    "    if pre_calculated:\n",
    "        train_features = np.load(join(output_path, 'fct_' + train_name + '_features.npy'))\n",
    "        train_targets = np.load(join(output_path, 'fct_' + train_name + '_target.npy'))\n",
    "        eval_features = np.load(join(output_path, 'fct_' + eval_name + '_features.npy'))\n",
    "        eval_targets = np.load(join(output_path, 'fct_' + eval_name + '_target.npy'))\n",
    "        edge_index = np.load(join(output_path, 'fct_edge_index.npy'))\n",
    "        edge_weights = np.load(join(output_path, 'fct_edge_weights.npy')) \n",
    "    else:\n",
    "        fe = FeatureEngineering (use_validation, feature_cols, training_end_date, validation_end_date, test_end_date)\n",
    "        train_df, eval_df, poi_columns = fe.get_datasets()     \n",
    "        print('Convert train set to Numpy arrays')\n",
    "        train_features, train_targets, node_id_to_index, timestamp_to_index = convert_df_to_numpy(train_df, feature_cols, poi_columns)\n",
    "        del timestamp_to_index\n",
    "        print('Convert evaluation set to Numpy arrays')\n",
    "        eval_features, eval_targets, test_node_id_to_index, test_timestamp_to_index = convert_df_to_numpy(eval_df, feature_cols, poi_columns)\n",
    "\n",
    "        print('Create edge info')\n",
    "        edges_df = pd.read_csv(join(output_path, \"interm_network_edges.csv\"), encoding='utf-8', sep=',')\n",
    "        edge_index, edge_weights = calculate_edges_info(edges_df, node_id_to_index, fe)\n",
    "\n",
    "        print('Save Numpy arrays')\n",
    "        if save:\n",
    "            np.save(join(output_path, 'fct_' + train_name + '_features.npy'), train_features)\n",
    "            np.save(join(output_path, 'fct_' + train_name + '_target.npy'), train_targets)\n",
    "            np.save(join(output_path, 'fct_' + eval_name + '_features.npy'), eval_features)\n",
    "            np.save(join(output_path, 'fct_' + eval_name + '_target.npy'), eval_targets)\n",
    "            np.save(join(output_path, 'fct_edge_index.npy'), edge_index)\n",
    "            np.save(join(output_path, 'fct_edge_weights.npy'), edge_weights)\n",
    "            if final_training:\n",
    "                np.save(join(output_path, 'fct_predicted_timestamps.npy'), test_timestamp_to_index)\n",
    "                save_dict(output_path, 'fct_predicted_nodes.npy', test_node_id_to_index)\n",
    "\n",
    "    num_features = train_features.shape[2]\n",
    "    print('Train Features Shape:', train_features.shape)\n",
    "    print('Train Targets Shape:', train_targets.shape)\n",
    "    print('Evaluation Features Shape:', eval_features.shape)\n",
    "    print('Evaluation Targets Shape:', eval_targets.shape)\n",
    "    print(\"Edge Index shape:\", edge_index.shape)\n",
    "    print(\"Edge Weights shape:\", edge_weights.shape)\n",
    "\n",
    "    train_dataloader, edge_index_tensor, edge_weights_tensor = static_graph_data_loader(train_features, train_targets, edge_index, edge_weights, 'Training', batch_size, periods)\n",
    "    eval_dataloader, edge_index_tensor, edge_weights_tensor = static_graph_data_loader(eval_features, eval_targets, edge_index, edge_weights, 'Evaluation', batch_size, periods)\n",
    "\n",
    "    return train_dataloader, eval_dataloader, edge_index_tensor, edge_weights_tensor, num_features\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9d7af8-310a-4c41-97b5-14a3eaa7c844",
   "metadata": {},
   "source": [
    "## Graph Neural Networks Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05a84bb5-25c5-48e7-94c9-cd96b3f7ab60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TemporalGNN(\n",
       "  (tgnn): A3TGCN2(\n",
       "    (_base_tgcn): TGCN2(\n",
       "      (conv_z): GCNConv(38, 128)\n",
       "      (linear_z): Linear(in_features=256, out_features=128, bias=True)\n",
       "      (conv_r): GCNConv(38, 128)\n",
       "      (linear_r): Linear(in_features=256, out_features=128, bias=True)\n",
       "      (conv_h): GCNConv(38, 128)\n",
       "      (linear_h): Linear(in_features=256, out_features=128, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (hidden_layers): ModuleList(\n",
       "    (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (3): ReLU()\n",
       "  )\n",
       "  (fc_mu): Linear(in_features=32, out_features=1, bias=True)\n",
       "  (fc_logvar): Linear(in_features=32, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TemporalGNN(torch.nn.Module):\n",
    "    def __init__(self, node_features, periods, batch_size, hidden_layers):\n",
    "        super(TemporalGNN, self).__init__()\n",
    "        \n",
    "        # Initial temporal graph convolution layer\n",
    "        self.tgnn = A3TGCN2(in_channels=node_features, out_channels=hidden_layers[0], periods=periods, batch_size=batch_size)\n",
    "\n",
    "        # Dynamically create hidden layers based on the 'hidden_layers' list\n",
    "        self.hidden_layers = torch.nn.ModuleList()\n",
    "        for i in range(1, len(hidden_layers)):\n",
    "            self.hidden_layers.append(Linear(hidden_layers[i - 1], hidden_layers[i]))\n",
    "            self.hidden_layers.append(ReLU())\n",
    "\n",
    "        # Output layers for mu and log variance\n",
    "        self.fc_mu = Linear(hidden_layers[-1], periods)\n",
    "        self.fc_logvar = Linear(hidden_layers[-1], periods)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight):\n",
    "        h = self.tgnn(x, edge_index)\n",
    "        h = F.relu(h)\n",
    "\n",
    "        # Pass through dynamically created hidden layers\n",
    "        for layer in self.hidden_layers:\n",
    "            h = layer(h)\n",
    "        \n",
    "        mu = self.fc_mu(h)\n",
    "        log_var = self.fc_logvar(h)\n",
    "        return mu, log_var\n",
    "\n",
    "\n",
    "TemporalGNN(node_features=38, periods=1, batch_size=64, hidden_layers=[128,64,32])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d101c19-731c-4f95-a05f-62afc3feeec3",
   "metadata": {},
   "source": [
    "## Probabilistic Forecasting Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ead84594-87ad-4022-b639-1563cee4a932",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_nll(mu, logvar, target):\n",
    "    return (0.5 * torch.exp(-logvar) * (target - mu) ** 2 + 0.5 * logvar).mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf2690b-9f09-44eb-92a7-d6316acb20a5",
   "metadata": {},
   "source": [
    "## Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b44cf8d1-6d58-4ccf-8ae5-fbdea8b72bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, eval_loader, loss_fn, edge_index_tensor, edge_weights_tensor):\n",
    "    epsilon = 1e-8\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        loss_list, mae_list, mape_list, r2_list, rmse_list = [], [], [], [], []\n",
    "\n",
    "        for encoder_inputs, labels in eval_loader:\n",
    "            # Forward pass through the model\n",
    "            mu, log_var = model(encoder_inputs, edge_index_tensor, edge_weights_tensor)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = gaussian_nll(mu, log_var, labels)\n",
    "            loss_list.append(loss.item())\n",
    "\n",
    "            predictions = mu\n",
    "            # Calculate MAE\n",
    "            mae = torch.mean(torch.abs(predictions - labels))\n",
    "            mae_list.append(mae.item())\n",
    "\n",
    "            # Calculate MAPE\n",
    "            mape = torch.mean(torch.abs(predictions - labels) / (torch.abs(labels) + epsilon))\n",
    "            mape_list.append(mape.detach().numpy().item())\n",
    "\n",
    "            # Calculate R2 Score\n",
    "            r2 = torchmetrics.functional.r2_score(predictions.view(-1), labels.view(-1))\n",
    "            r2_list.append(r2.item())\n",
    "\n",
    "            # Calculate RMSE\n",
    "            rmse = torch.sqrt(torch.mean(torch.pow(predictions - labels, 2)))\n",
    "            rmse_list.append(rmse.item())\n",
    "\n",
    "        # Aggregate metrics\n",
    "        avg_loss = sum(loss_list) / len(loss_list)\n",
    "        avg_mae = sum(mae_list) / len(mae_list)\n",
    "        avg_mape = sum(mape_list) / len(mape_list)\n",
    "        avg_r2 = sum(r2_list) / len(r2_list)\n",
    "        avg_rmse = sum(rmse_list) / len(rmse_list)\n",
    "        avg_rmse = sum(rmse_list) / len(rmse_list)\n",
    "\n",
    "    return avg_loss, avg_mae, avg_mape, avg_r2, avg_rmse\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93426e89-d174-4030-a7e6-ba9f739cc9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_training(train_dataloader, eval_dataloader, epochs, num_features, periods, batch_size, hidden_layers, lr, edge_index_tensor, edge_weights_tensor, validation_mode):\n",
    "\n",
    "    model=TemporalGNN(node_features=num_features, periods=periods, batch_size=batch_size, hidden_layers=hidden_layers)\n",
    "    optimizer = torch.optim.RMSprop(model.parameters(), lr=lr)\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    model.train()\n",
    "    epsilon = 1e-8\n",
    "    \n",
    "    train_loss_ls, train_mae_ls, train_r2_ls, train_rmse_ls, train_mape_ls = [], [], [], [], []\n",
    "    eval_loss_ls, eval_mae_ls, eval_r2_ls, eval_rmse_ls, eval_mape_ls = [], [], [], [], []\n",
    "    \n",
    "    for epoch in range(0,epochs):\n",
    "        loss_list, mae_list ,r2_list, rmse_list, mape_list = [], [], [], [], []\n",
    "        step = 0\n",
    "        for encoder_inputs, labels in tqdm(train_dataloader): \n",
    "            mu, log_var = model(encoder_inputs, edge_index_tensor, edge_weights_tensor)       # Get model predictions\n",
    "            loss = gaussian_nll(mu, log_var, labels)\n",
    "            loss.backward()\n",
    "            loss_list.append(loss.item())\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            step = step + 1\n",
    "            \n",
    "            y_pred = mu\n",
    "            y_true = labels\n",
    "            \n",
    "            mae = torch.mean(torch.abs(y_pred - y_true))\n",
    "            mae_list.append(mae.detach().numpy().item())\n",
    "\n",
    "            mape = torch.mean(torch.abs(y_pred - y_true) / (torch.abs(y_true) + epsilon))\n",
    "            mape_list.append(mape.detach().numpy().item())\n",
    "            \n",
    "            r2 = torchmetrics.functional.r2_score(y_pred.view(-1), y_true.view(-1))\n",
    "            r2_list.append(r2.detach().numpy().item())\n",
    "            \n",
    "            rmse = torch.sqrt(torch.mean(torch.pow(y_pred - y_true, 2)))\n",
    "            rmse_list.append(rmse.detach().numpy().item())\n",
    "    \n",
    "        train_loss = sum(loss_list) / len(loss_list)\n",
    "        train_mae = sum(mae_list) / len(mae_list)\n",
    "        train_mape = sum(mape_list) / len(mape_list)\n",
    "        train_r2 = sum(r2_list) / len(r2_list)\n",
    "        train_rmse = sum(rmse_list) / len(rmse_list)\n",
    "        \n",
    "        print(\"Epoch {}, Train || NLL: {:.7f}, MAE: {:.7f}, MAPE: {:.7f}, R2: {:.7f}, RMSE: {:.7f}\".format(epoch+1,train_loss,train_mae,train_mape,train_r2,train_rmse))\n",
    "        train_loss_ls.append(train_loss)\n",
    "        train_mae_ls.append(train_mae)\n",
    "        train_mape_ls.append(train_mape)\n",
    "        train_r2_ls.append(train_r2)\n",
    "        train_rmse_ls.append(train_rmse)\n",
    "\n",
    "        if validation_mode:\n",
    "            eval_loss,eval_mae,eval_mape,eval_r2,eval_rmse = evaluate_model(model, eval_dataloader, loss_fn, edge_index_tensor, edge_weights_tensor)\n",
    "            print(\"Epoch {}, Evaluation || NLL: {:.7f}, MAE: {:.7f}, MAPE: {:.7f}, R2: {:.7f}, RMSE: {:.7f}\".format(epoch+1,eval_loss,eval_mae,eval_mape,eval_r2,eval_rmse))\n",
    "            eval_loss_ls.append(eval_loss)\n",
    "            eval_mae_ls.append(eval_mae)\n",
    "            eval_mape_ls.append(eval_mape)\n",
    "            eval_r2_ls.append(eval_r2)\n",
    "            eval_rmse_ls.append(eval_rmse)\n",
    "\n",
    "    metrics = {'train_nll_loss_ls': train_loss_ls,'train_mae_ls': train_mae_ls,'train_mape_ls': train_mape_ls,'train_r2_ls': train_r2_ls,\n",
    "               'train_rmse_ls': train_rmse_ls,'eval_nll_loss_ls': eval_loss_ls,'eval_mae_ls': eval_mae_ls,'eval_mape_ls': eval_mape_ls,\n",
    "               'eval_r2_ls': eval_r2_ls,'eval_rmse_ls': eval_rmse_ls}\n",
    "    return model, metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f7981ee-3886-462e-bfce-b2c36c10a8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_forecasts(model, eval_dataloader, edge_index_tensor, edge_weights_tensor, node_id_mapping, timestamps):\n",
    "    \"\"\"\n",
    "    Generates forecasts for the given model and dataloader, associating predictions with node IDs and timestamps.\n",
    "    \n",
    "    Parameters:\n",
    "        model (torch.nn.Module): Trained model ready for predictions.\n",
    "        eval_dataloader (DataLoader): DataLoader containing the evaluation data.\n",
    "        edge_index_tensor (Tensor): Edge indices for the graph.\n",
    "        edge_weights_tensor (Tensor): Edge weights for the graph.\n",
    "        node_id_mapping (dict): Mapping of node indices to node IDs.\n",
    "        timestamps (np.array): Array of timestamps corresponding to the evaluations.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing node IDs, timestamps, mu, and log_var.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    forecasts = []\n",
    "    index_to_node_id = {v: k for k, v in node_id_mapping.items()}\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (encoder_inputs, _) in enumerate(eval_dataloader):\n",
    "            mu, log_var = model(encoder_inputs, edge_index_tensor, edge_weights_tensor)\n",
    "\n",
    "            # Iterate over each timestamp and node in the batch\n",
    "            for idx in range(mu.size(0)):\n",
    "                for node_idx in range(mu.size(1)):\n",
    "                    node_id = index_to_node_id[node_idx]  # Correctly map index back to node ID using the inverted dictionary\n",
    "                    timestamp = timestamps[batch_idx * eval_dataloader.batch_size + idx]\n",
    "                    forecasts.append({\n",
    "                        \"node_id\": node_id,\n",
    "                        \"timestamp\": timestamp,\n",
    "                        \"mean\": mu[idx, node_idx].item(),\n",
    "                        \"log_var\": log_var[idx, node_idx].item()\n",
    "                    })\n",
    "\n",
    "    return pd.DataFrame(forecasts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4f5d41-0ecc-4a9f-b581-718c60ef63fe",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "38571c7b-7fbc-4e55-a198-ee6f0cf15611",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dict(path, filename, data):\n",
    "    with open(path + '/' + filename + '.json', 'w') as f:\n",
    "        json.dump(data, f)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "735a731b-c8ed-4557-8714-440faa6928fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dict(path, filename):\n",
    "    with open(path + '/' + filename + '.json', 'r') as f:\n",
    "        data_loaded = json.load(f)\n",
    "    return data_loaded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1469f1-e28a-4521-9169-9f2b4f723b01",
   "metadata": {},
   "source": [
    "## Execution Varibales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf32fe7e-3bb1-4b2d-acb1-34216423a974",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_end_date = '2023-01-01'\n",
    "validation_end_date = '2024-01-01'\n",
    "test_end_date = '2025-01-10'\n",
    "use_validation = True\n",
    "pre_calculated = False\n",
    "save = True\n",
    "periods = 1\n",
    "batch_size = 32\n",
    "\n",
    "train_dataloader, eval_dataloader, edge_index, edge_weights, num_features = create_graph_data(use_validation, training_end_date, validation_end_date, test_end_date, output_path, pre_calculated, save, feature_cols, batch_size, periods)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441c74a5-af39-4141-8fbb-11d5b58acdc0",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7451b45d-85be-4123-9c78-e1d4fbdbca75",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_end_date = '2023-01-01'\n",
    "validation_end_date = '2024-01-01'\n",
    "test_end_date = '2025-01-10'\n",
    "use_validation = True\n",
    "pre_calculated = True\n",
    "save = False\n",
    "periods = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08959a71-ae90-4770-b1c7-493d2be0cc48",
   "metadata": {},
   "source": [
    "## Experiments With Network Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b9bbb2-00f3-47e8-b8d6-c54cafff65cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hidden_layers_ls = [ [64], [128,64], [128,64,32], [256,128,64,32], [512,256,128,64,32] ]\n",
    "model_name_ls = ['A3TGCN2', 'A3TGCN2_1hid', 'A3TGCN2_2hid', 'A3TGCN2_3hid', 'A3TGCN2_4hid']\n",
    "batch_size = 32\n",
    "epochs = 60\n",
    "lr = 0.001\n",
    "\n",
    "train_dataloader, eval_dataloader, edge_index, edge_weights, num_features = create_graph_data(use_validation, training_end_date, validation_end_date, test_end_date, output_path, pre_calculated, save, feature_cols, batch_size, periods)\n",
    "\n",
    "for i in range (0,len(model_name_ls)):\n",
    "    hidden_layers = hidden_layers_ls[i]\n",
    "    print('Experiment with Model = ', model_name_ls[i])\n",
    "    experiment_name = 'experiment_model_' + str(model_name_ls[i])\n",
    "    model, metrics = model_training(train_dataloader, eval_dataloader, epochs, num_features, periods, batch_size, hidden_layers, lr, edge_index, edge_weights, True)\n",
    "    save_dict(metrics_path, experiment_name, metrics)\n",
    "    print('\\n\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72740a16-e002-400f-9ace-81c51faa25ab",
   "metadata": {},
   "source": [
    "## Experiments With Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0877df2a-7b9d-4004-b197-b8050b4a0780",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layers = [256,128,64,32]\n",
    "epochs = 120\n",
    "batch_size = 32\n",
    "lr = 0.001\n",
    "\n",
    "train_dataloader, eval_dataloader, edge_index, edge_weights, num_features = create_graph_data(use_validation, training_end_date, validation_end_date, test_end_date, output_path, pre_calculated, save, feature_cols, batch_size, periods)\n",
    "model, metrics = model_training(train_dataloader, eval_dataloader, epochs, num_features, periods, batch_size, hidden_layers, lr, edge_index, edge_weights, True)\n",
    "experiment_name = 'experiment_epochs_' + str(epochs)\n",
    "save_dict(metrics_path, experiment_name, metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9cb35e-7fc8-48d2-b1b1-0c8d093447c7",
   "metadata": {},
   "source": [
    "## Experiments with Batch Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ab7c39-4df0-4f64-8e48-72b9de9bc074",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layers = [256,128,64,32]\n",
    "batch_size_ls = [8, 16, 32, 64]\n",
    "\n",
    "epochs = 60\n",
    "lr = 0.001\n",
    "\n",
    "for batch_size in batch_size_ls:\n",
    "    print('Experiment with batch size = ', batch_size)\n",
    "    experiment_name = 'experiment_batch_size_' + str(batch_size)\n",
    "    train_dataloader, eval_dataloader, edge_index, edge_weights, num_features = create_graph_data(use_validation, training_end_date, validation_end_date, test_end_date, output_path, pre_calculated, save, feature_cols, batch_size, periods)\n",
    "    model, metrics = model_training(train_dataloader, eval_dataloader, epochs, num_features, periods, batch_size, hidden_layers, lr, edge_index, edge_weights, True)\n",
    "    save_dict(metrics_path, experiment_name, metrics)\n",
    "    print('\\n\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18a01ae-eb80-401f-baf5-d06bcafbe187",
   "metadata": {},
   "source": [
    "## Experiments with Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d76b82-3168-4d56-ba6b-246b1875e3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layers = [256,128,64,32]\n",
    "batch_size_ls = 32\n",
    "epochs = 60\n",
    "learning_rate_ls = [0.01, 0.001, 0.0001]\n",
    "\n",
    "train_dataloader, eval_dataloader, edge_index, edge_weights, num_features = create_graph_data(use_validation, training_end_date, validation_end_date, test_end_date, output_path, pre_calculated, save, feature_cols, batch_size, periods)\n",
    "\n",
    "for lr in learning_rate_ls:\n",
    "    print('Experiment with Learning Rate = ', lr)\n",
    "    experiment_name = 'experiment_learning_rate_' + str(lr)\n",
    "    model, metrics = model_training(train_dataloader, eval_dataloader, epochs, num_features, periods, batch_size, hidden_layers, lr, edge_index, edge_weights, True)\n",
    "    save_dict(metrics_path, experiment_name, metrics)\n",
    "    print('\\n\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d23371-7f07-481f-808e-4bf2a6232390",
   "metadata": {},
   "source": [
    "## Ablation Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb80c0f1-2a43-4723-9682-44546b7ce82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layers = [256,128,64,32]\n",
    "pre_calculated = False\n",
    "save = False\n",
    "feature_names = ['core_features', 'calendar_features', 'rolling_avg_features', 'lag_features']\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 60\n",
    "lr = 0.001\n",
    "\n",
    "for feature in feature_names:\n",
    "\n",
    "    if feature == 'core_features':\n",
    "        feature_cols = core_features\n",
    "    elif feature == 'calendar_features':\n",
    "        feature_cols = core_features + calendar_features\n",
    "    elif feature == 'rolling_avg_features':\n",
    "        feature_cols = core_features + calendar_features + rolling_avg_features\n",
    "    elif feature == 'lag_features':\n",
    "        feature_cols = core_features + calendar_features + rolling_avg_features + lag_features\n",
    "\n",
    "    print('Experiment with features = ', feature)\n",
    "    experiment_name = 'experiment_features_' + str(feature)\n",
    "    train_dataloader, eval_dataloader, edge_index, edge_weights, num_features = create_graph_data(use_validation, training_end_date, validation_end_date, test_end_date, output_path, pre_calculated, save, feature_cols, batch_size, periods)\n",
    "    model, metrics = model_training(train_dataloader, eval_dataloader, epochs, num_features, periods, batch_size, hidden_layers, lr, edge_index, edge_weights, True)\n",
    "    save_dict(metrics_path, experiment_name, metrics)\n",
    "    print('\\n\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b12b359-59dc-40d0-9f34-2adf79e086e6",
   "metadata": {},
   "source": [
    "## Calendar Features Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb4af6e-47ba-4348-bc74-090cb4e7e289",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layers = [256,128,64,32]\n",
    "pre_calculated = False\n",
    "save = False\n",
    "feature_names = ['hour_quarter', 'dayofweek', 'month', 'dayofmonth', 'weekofyear','all']\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 60\n",
    "lr = 0.001\n",
    "\n",
    "for feature in feature_names:\n",
    "\n",
    "    if feature == 'hour_quarter':\n",
    "        feature_cols = core_features + ['sin_hour','cos_hour','sin_quarter_hour','cos_quarter_hour']\n",
    "    elif feature == 'dayofweek':\n",
    "        feature_cols = core_features + ['sin_hour','cos_hour','sin_quarter_hour','cos_quarter_hour','sin_dayofweek','cos_dayofweek']\n",
    "    elif feature == 'month':\n",
    "        feature_cols = core_features + ['sin_hour','cos_hour','sin_quarter_hour','cos_quarter_hour','sin_dayofweek','cos_dayofweek','sin_month','cos_month']\n",
    "    elif feature == 'dayofmonth':\n",
    "        feature_cols = core_features + ['sin_hour','cos_hour','sin_quarter_hour','cos_quarter_hour','sin_dayofweek','cos_dayofweek','sin_month','cos_month','sin_dayofmonth','cos_dayofmonth']\n",
    "    elif feature == 'weekofyear':\n",
    "        feature_cols = core_features + ['sin_hour','cos_hour','sin_quarter_hour','cos_quarter_hour','sin_dayofweek','cos_dayofweek','sin_month','cos_month','sin_dayofmonth','cos_dayofmonth','sin_weekofyear','cos_weekofyear']\n",
    "    elif feature == 'all':\n",
    "        feature_cols = core_features + ['sin_hour','cos_hour','sin_quarter_hour','cos_quarter_hour','sin_dayofweek','cos_dayofweek','sin_month','cos_month','sin_dayofmonth','cos_dayofmonth','sin_weekofyear','cos_weekofyear','sin_year','cos_year']\n",
    "\n",
    "    print('Experiment with calendar features = ', feature)\n",
    "    experiment_name = 'experiment_calendar_features_' + str(feature)\n",
    "    train_dataloader, eval_dataloader, edge_index, edge_weights, num_features = create_graph_data(use_validation, training_end_date, validation_end_date, test_end_date, output_path, pre_calculated, save, feature_cols, batch_size, periods)\n",
    "    model, metrics = model_training(train_dataloader, eval_dataloader, epochs, num_features, periods, batch_size, hidden_layers, lr, edge_index, edge_weights, True)\n",
    "    save_dict(metrics_path, experiment_name, metrics)\n",
    "    print('\\n\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238971c5-7b7b-4b06-a3f4-e6dff9643190",
   "metadata": {},
   "source": [
    "## Experiments with Testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89360066-111c-4dbf-94a3-87ba6addf357",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_end_date = '2023-01-01'\n",
    "validation_end_date = '2024-01-01'\n",
    "test_end_date = '2025-01-10'\n",
    "\n",
    "use_validation = False\n",
    "pre_calculated = True\n",
    "save = False\n",
    "periods = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b16918-538e-455e-86d2-a67a3448824e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layers = [256,128,64,32]\n",
    "epochs = 80\n",
    "batch_size = 32\n",
    "lr = 0.001\n",
    "\n",
    "train_dataloader, eval_dataloader, edge_index, edge_weights, num_features = create_graph_data(use_validation, training_end_date, validation_end_date, test_end_date, output_path, pre_calculated, save, feature_cols, batch_size, periods)\n",
    "model, metrics = model_training(train_dataloader, eval_dataloader, epochs, num_features, periods, batch_size, hidden_layers, lr, edge_index, edge_weights, True)\n",
    "experiment_name = 'final_model_test_metrics'\n",
    "save_dict(metrics_path, experiment_name, metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fee10be-d8a6-4b84-8123-363c8d3b55ea",
   "metadata": {},
   "source": [
    "## Full Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bd3a46-4504-4153-9b5c-b4ca5e5c67bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_end_date = '2023-01-01'\n",
    "validation_end_date = '2025-01-09'\n",
    "test_end_date = '2025-01-10'\n",
    "\n",
    "final_training = True\n",
    "use_validation = False\n",
    "pre_calculated = True\n",
    "save = False\n",
    "periods = 1\n",
    "\n",
    "hidden_layers = [256,128,64,32]\n",
    "epochs = 20\n",
    "batch_size = 32\n",
    "lr = 0.001\n",
    "\n",
    "train_dataloader, eval_dataloader, edge_index, edge_weights, num_features = create_graph_data(use_validation, training_end_date, validation_end_date, test_end_date, output_path, pre_calculated, save, feature_cols, batch_size, periods, final_training)\n",
    "model, metrics = model_training(train_dataloader, eval_dataloader, epochs, num_features, periods, batch_size, hidden_layers, lr, edge_index, edge_weights, False)\n",
    "torch.save(model.state_dict(), join(gnn_model_path, \"TGCN_model.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b553108-f1df-4cb7-a76d-d01365b7c032",
   "metadata": {},
   "source": [
    "## Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4643eff6-4acb-4c5c-a9ce-7fb5f3775e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Features Shape: (75072, 225, 41)\n",
      "Train Targets Shape: (75072, 225, 1)\n",
      "Evaluation Features Shape: (68, 225, 41)\n",
      "Evaluation Targets Shape: (68, 225, 1)\n",
      "Edge Index shape: (2, 292)\n",
      "Edge Weights shape: (292,)\n",
      "Training Features tensor shape: torch.Size([75072, 225, 41, 1])\n",
      "Training Targets tensor shape: torch.Size([75072, 225, 1])\n",
      "Evaluation Features tensor shape: torch.Size([68, 225, 41, 1])\n",
      "Evaluation Targets tensor shape: torch.Size([68, 225, 1])\n"
     ]
    }
   ],
   "source": [
    "training_end_date = '2023-01-01'\n",
    "validation_end_date = '2025-01-09'\n",
    "pred_date = '2025-01-10'\n",
    "\n",
    "final_training = True\n",
    "use_validation = False\n",
    "pre_calculated = True\n",
    "save = False\n",
    "\n",
    "hidden_layers = [256,128,64,32]\n",
    "batch_size = 32\n",
    "periods = 1\n",
    "\n",
    "train_dataloader, pred_dataloader, edge_index, edge_weights, num_features = create_graph_data(use_validation, training_end_date, validation_end_date, pred_date, output_path, pre_calculated, save, feature_cols, batch_size, periods, final_training)\n",
    "timestamps = np.load(join(output_path, \"fct_predicted_timestamps.npy\"), allow_pickle=True)\n",
    "node_id_mapping = load_dict(output_path, \"fct_predicted_nodes\")\n",
    "\n",
    "pre_trained_model = TemporalGNN(node_features=num_features, periods=periods, batch_size=batch_size, hidden_layers=hidden_layers)\n",
    "pre_trained_model.load_state_dict(torch.load(join(gnn_model_path, \"TGCN_model.pt\")))\n",
    "\n",
    "forecast_df = generate_forecasts(pre_trained_model, pred_dataloader, edge_index, edge_weights, node_id_mapping, timestamps)\n",
    "forecast_df.to_csv(join(output_path, \"interm_avg_speed_forecasts.csv\"), sep=',', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d2acc1-00f6-4797-9b51-ad5426fcf7c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Torch (bus_sch))",
   "language": "python",
   "name": "torch_geometric"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
