{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db733ee1",
   "metadata": {},
   "source": [
    "<h1>\n",
    "<center>Probabilistic Road Traffic Forecasting with Common Machine Learning Classifiers</center>\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0dc6378",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "As mentioned in the report, our goal is to develop a probabilistic forecasting LSTM model for item-store predictions. Currently, in the field of Automation Supply Chain, there is a significant interest in quantitative supply chain approaches that employ probabilistic forecasting. About 95% of these methods generate forecasts using machine learning classifiers with the predict_proba method. In a previous notebbok, we proposed an LSTM model as a superior alternative to classifiers for obtaining probabilities. To validate this, we have created this notebook where we use common machine learning classifiers for probabilistic demand forecasting and measure the results. \n",
    "<br>\n",
    "<br>\n",
    "Αiming to make as fair a comparison as possible, we perform hyper-parameter optimization οn every algorithm using the validation set. Then knowing the best combination of hyper-parameters we train each model and evaluate the model on test set by measuring various metrics\n",
    "<br>\n",
    "<br> \n",
    "We obtain the probabilistic forecasts in a distribution format from classifiers, calculate the mean of each output distribution, and compare it with the actual sales recorded. Thus, we treat the mean of the distribution and the actual sales as values and apply regression metrics such as MAE, MSE, MAPE, and R2. This explains why we use regression metrics on classifier outputs.\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a84bcd",
   "metadata": {},
   "source": [
    "## Generals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e92272",
   "metadata": {},
   "source": [
    "<font size=\"3\"> \n",
    "Packages import and system configurations. \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab887616",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data\n",
    "import os\n",
    "from os.path import join\n",
    "import time\n",
    "import numpy as np\n",
    "import itertools\n",
    "from data_handler import FeatureEngineering\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "import json\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier  \n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier  # For Random Forest, Gradient Boosting, and AdaBoost\n",
    "from sklearn.neighbors import KNeighborsClassifier  \n",
    "from sklearn.neural_network import MLPClassifier  \n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55210b91",
   "metadata": {},
   "source": [
    "<font size=\"3\"> \n",
    "Define necessary paths. \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056cf21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = join(\"io\", \"input\")\n",
    "output_path = join(\"io\", \"output\")\n",
    "experiments_path = join(\"io\", \"experiments\")\n",
    "metrics_path = join(experiments_path, \"metrics\")\n",
    "plots_path = join(experiments_path, \"plots\")\n",
    "\n",
    "plots_path\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c4c99b",
   "metadata": {},
   "source": [
    "## Core Functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfd3ae0",
   "metadata": {},
   "source": [
    "<font size=\"3\"> \n",
    "Load data according to use_validation (if true: return train-validation | else: return train - test) \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3295d6ad-e8d2-41ed-9cdc-5256fd2608b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(use_validation, feature_cols, training_end_date, validation_end_date, test_end_date):\n",
    "    \n",
    "    fe = FeatureEngineering (use_validation, feature_cols, training_end_date, validation_end_date, test_end_date)\n",
    "    train_set, eval_set, poi_columns = fe.get_datasets()\n",
    "    x_train, y_train = split_x_y(train_set.head(100), feature_cols+poi_columns) \n",
    "    x_eval, y_eval = split_x_y(eval_set.head(100), feature_cols+poi_columns)\n",
    "    original_classes = sorted(np.unique(y_train))\n",
    "    desired_classes = list(range(len(original_classes)))\n",
    "    y_train = [desired_classes[original_classes.index(c)] for c in y_train]\n",
    "  \n",
    "    return x_train, y_train, x_eval, y_eval, original_classes\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f282e04f",
   "metadata": {},
   "source": [
    "<font size=\"3\"> \n",
    "Split: features -  target\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f5b8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_x_y(df, feature_cols):\n",
    "    feature_cols = [col for col in feature_cols if col!='node_id']\n",
    "    y = df['target']\n",
    "    x = df[feature_cols]\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ae6d7f",
   "metadata": {},
   "source": [
    "## Hyper-Parameters Tunig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae83664",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "A core function that apply the evaluation procces using train & validation set:\n",
    "<ol>\n",
    "<li>Initialize the given Classification models (KNN, DecisionTree, XGBoost & more) with default hyperparameters.</li>\n",
    "<li>Define the hyperparameter search spaces (ranges of values to try for each hyperparameter) for each model.</li>\n",
    "<li>Loop through the three models and their corresponding hyperparameter search spaces.</li>\n",
    "<li>Generate all possible combinations of hyperparameters for each model.</li>\n",
    "<li>For each hyperparameter combination, fit the model on the training set, make predictions on the validation set, and calculate the MAE score.</li>\n",
    "<li>Track the best hyperparameters for each model based on the lowest MAE score on the validation set.</li>\n",
    "<li>Return the best hyperparameters for each model as a list of dictionaries.</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb304cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c37096f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_tuning_validation(x_train, y_train, x_val, y_val, original_classes):\n",
    "    # Initialize the models\n",
    "    tree_cl = DecisionTreeClassifier()\n",
    "    xgb_cl = XGBClassifier(objective='multi:softprob')\n",
    "    lm_cl = RidgeClassifier()\n",
    "    knn_cl = KNeighborsClassifier(weights='distance')\n",
    "    rf_cl = RandomForestClassifier()\n",
    "    gb_cl = GradientBoostingClassifier()\n",
    "    ada_cl = AdaBoostClassifier()\n",
    "    mpl_cl = MLPClassifier()\n",
    "\n",
    "    # Define the hyperparameter ranges\n",
    "    tree_param_grid = {'criterion':['gini', 'entropy'],'max_depth': [12,16,32], 'min_samples_split': [6, 8, 16]}\n",
    "    xgb_param_grid = {'n_estimators': [6, 12, 24], 'max_depth': [3, 6, 12], 'learning_rate':[0.3,0.5,0.8]}\n",
    "    lm_param_grid = {'alpha': [0.1, 1.0, 10.0]}\n",
    "    knn_param_grid = {'n_neighbors': [3, 5, 9, 51, 99], 'algorithm': ['ball_tree', 'kd_tree', 'brute']}\n",
    "    rf_param_grid = {'max_depth': [8, 12, 24], 'n_estimators': [12, 16, 32], 'min_samples_split': [4, 8, 16]}\n",
    "    gb_param_grid = {'learning_rate': [0.01, 0.1, 0.5], 'n_estimators': [16, 24, 48], 'max_depth': [3, 6, 12]}\n",
    "    ada_param_grid = {'learning_rate': [0.001, 0.01, 0.1], 'n_estimators': [30, 40, 60]}\n",
    "    mpl_param_grid = {'hidden_layer_sizes': [(50,50,50), (50,100,50), (100,)], 'activation': ['tanh', 'relu'], 'solver': ['sgd', 'adam'], 'learning_rate': ['constant','adaptive']}\n",
    "\n",
    "    \n",
    "    models = [tree_cl, xgb_cl, lm_cl, knn_cl, rf_cl, gb_cl, ada_cl, mpl_cl]\n",
    "    param_grids = [tree_param_grid, xgb_param_grid, lm_param_grid,\n",
    "                  knn_param_grid, rf_param_grid, gb_param_grid, ada_param_grid, mpl_param_grid]\n",
    "\n",
    "    best_params = []\n",
    "    for i, model in enumerate(models):\n",
    "        print('\\n')\n",
    "        best_mae = float('inf')\n",
    "        best_mape = float('inf')\n",
    "        best_params_i = {}\n",
    "        # Generate all possible combinations of hyperparameters\n",
    "        param_combinations = list(itertools.product(*(param_grids[i][param] for param in param_grids[i])))\n",
    "        for j, params in enumerate(param_combinations):\n",
    "            # Unpack the tuple of parameter values into individual arguments\n",
    "            params_dict = dict(zip(param_grids[i], params))\n",
    "            model.set_params(**params_dict)\n",
    "            model.fit(x_train, y_train)\n",
    "            \n",
    "            if model.__class__.__name__ == 'RidgeClassifier':\n",
    "                y_val_pred_proba = model.decision_function(x_val)\n",
    "            else: \n",
    "                y_val_pred_proba = model.predict_proba(x_val)\n",
    "\n",
    "            y_val_pred = np.dot(y_val_pred_proba, original_classes)\n",
    "            mse = round(mean_squared_error(y_val, y_val_pred), 7)\n",
    "            mae = round(mean_absolute_error(y_val, y_val_pred),7)\n",
    "            mape = round(mean_absolute_percentage_error(y_val, y_val_pred), 7)\n",
    "            r2 = round(r2_score(y_val, y_val_pred),7)\n",
    "            mse = round(mean_squared_error(y_val, y_val_pred), 7)\n",
    "            param_str = ', '.join([f'{param}={value}' for param, value in params_dict.items()])\n",
    "            print(f\"Experiment {j+1} with {model.__class__.__name__} using {param_str} has MAE: {mae}, MAPE: {mape}, MSE: {mse}, R2: {r2}\")\n",
    "            if mae < best_mae:\n",
    "                best_mae = mae\n",
    "                best_r2 = r2\n",
    "                best_params_i = dict(zip(param_grids[i], params))\n",
    "        best_params.append(best_params_i)\n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f853f6a9",
   "metadata": {},
   "source": [
    "## Model Train & Test Evalaution "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7330354",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "A core function that apply the final train and evaluation procces using train & test set:\n",
    "<ol>\n",
    "<li>Initialize the models with the best hyperparameters.</li>\n",
    "<li>Fit each model on the training data.</li>\n",
    "<li>Use the fitted models to make predictions on the test set.</li>\n",
    "<li>Compute the evaluation metrics (MSE, MAE, R2, MAPE) for each model.</li>\n",
    "<li>Store the results for each model in a dictionary.</li>\n",
    "<li>Print the evaluation metrics for each model.</li>\n",
    "<li>Return the dictionary containing the results.</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3397e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(x_train, y_train, x_test, y_test, original_classes, best_params):\n",
    "    # Initialize the models with the best hyperparameters\n",
    "    tree_cl = DecisionTreeClassifier(**best_params[0])\n",
    "    xgb_cl = XGBClassifier(objective='multi:softprob', **best_params[1])\n",
    "    lm_cl = RidgeClassifier(**best_params[2])\n",
    "    knn_cl = KNeighborsClassifier(weights='distance', **best_params[3])\n",
    "    rf_cl = RandomForestClassifier(**best_params[4])\n",
    "    gb_cl = GradientBoostingClassifier(**best_params[5])\n",
    "    ada_cl = AdaBoostClassifier(**best_params[6])\n",
    "    mpl_cl = MLPClassifier(**best_params[7])\n",
    "\n",
    "    models = [tree_cl, xgb_cl, lm_cl, knn_cl, rf_cl, gb_cl, ada_cl, mpl_cl]\n",
    "    model_names = ['Decision Tree', 'XGBoost', 'Ridge Classifier',\n",
    "                  'KNN', 'Random Forest', 'Gradient Boosting', 'AdaBoost', 'MLP Classifier']\n",
    "    \n",
    "    results = {}\n",
    "    for i, model in enumerate(models):\n",
    "        start_time = time.time()\n",
    "        model.fit(x_train, y_train)\n",
    "        \n",
    "        if model.__class__.__name__ == 'RidgeClassifier':\n",
    "            y_test_pred_proba = model.decision_function(x_test)\n",
    "        else: \n",
    "            y_test_pred_proba = model.predict_proba(x_test)\n",
    "                \n",
    "        y_test_pred = np.dot(y_test_pred_proba, original_classes)\n",
    "        end_time = time.time()\n",
    "        execution_time = end_time - start_time\n",
    "        mse = round(mean_squared_error(y_test, y_test_pred), 7)\n",
    "        mae = round(mean_absolute_error(y_test, y_test_pred),7)\n",
    "        r2 = round(r2_score(y_test, y_test_pred),7)\n",
    "        mape = round(mean_absolute_percentage_error(y_test, y_test_pred), 7)\n",
    "        results[model_names[i]] = {'MAE': mae,'MAPE': mape, 'MSE': mse, 'R2': r2, 'Execution Time': execution_time}\n",
    "        print(f\"{model_names[i]} model has MAE: {mae}, MAPE: {mape}, MSE: {mse}, R2: {r2}, 'Execution Time': {execution_time}\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6414bf4",
   "metadata": {},
   "source": [
    "<font size=\"3\"> \n",
    "Plot metrics on subplots for camparison purposes\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0a7957",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_metrics_train_val(results,metrics_plot_path):\n",
    "    model_names = list(results.keys())\n",
    "    mse = [results[model]['MSE'] for model in model_names]\n",
    "    mae = [results[model]['MAE'] for model in model_names]\n",
    "    mape = [results[model]['MAPE'] for model in model_names]\n",
    "    r2 = [results[model]['R2'] for model in model_names]\n",
    "    \n",
    "    x = np.arange(len(model_names))\n",
    "    width = 0.35\n",
    "    \n",
    "    fig, axs = plt.subplots(nrows=2, ncols=2,figsize=(12, 12))\n",
    "    fig.suptitle('Comparison of Model Performance', fontsize=16)\n",
    "    # Plot the first metric on the top-left subplot\n",
    "    axs[0, 0].bar(x - width/2, mse, width, label='MSE')\n",
    "    axs[0, 0].set_ylabel('MSE')\n",
    "    axs[0, 0].set_title('MSE')\n",
    "    axs[0, 0].set_xticks(x)\n",
    "    axs[0, 0].set_xticklabels(model_names,rotation=90)\n",
    "    #axs[0, 0].set_ylim(0,1.2)\n",
    "    axs[0, 0].legend()\n",
    "    # Plot the second metric on the top-right subplot\n",
    "    axs[0, 1].bar(x - width/2, mae, width, label='MAE')\n",
    "    axs[0, 1].set_ylabel('MAE')\n",
    "    axs[0, 1].set_title('MAE')\n",
    "    axs[0, 1].set_xticks(x)\n",
    "    axs[0, 1].set_xticklabels(model_names,rotation=90)\n",
    "    #axs[0, 1].set_ylim(0,1.2)\n",
    "    axs[0, 1].legend()\n",
    "    # Plot the third metric on the bottom-left subplot\n",
    "    axs[1, 0].bar(x - width/2, mape, width, label='MAPE')\n",
    "    axs[1, 0].set_ylabel('MAPE')\n",
    "    axs[1, 0].set_title('MAPE')\n",
    "    axs[1, 0].set_xticks(x)\n",
    "    axs[1, 0].set_xticklabels(model_names,rotation=90)\n",
    "    #axs[1, 0].set_ylim(0,1.2)\n",
    "    axs[1, 0].legend()\n",
    "    # Plot the fourth metric on the bottom-right subplot\n",
    "    axs[1, 1].bar(x - width/2, r2, width, label='R2')\n",
    "    axs[1, 1].set_ylabel('R2')\n",
    "    axs[1, 1].set_title('R2')\n",
    "    axs[1, 1].set_xticks(x)\n",
    "    axs[1, 1].set_xticklabels(model_names,rotation=90)\n",
    "    #axs[1, 1].set_ylim(0,1.2)\n",
    "    axs[1, 1].legend()\n",
    "    # Adjust the spacing between subplots\n",
    "    plt.tight_layout()\n",
    "    # Show the plot\n",
    "    plt.savefig(metrics_plot_path + '/common_algo_test_metrics.pdf')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a03ada1-2092-4e64-b0f6-7b9d37b3a690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dict(path, filename, data):\n",
    "    with open(path + '/' + filename + '.json', 'w') as f:\n",
    "        json.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fee452f-e975-4b74-a090-f52d2d16b513",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dict(path, filename):\n",
    "    with open(path + '/' + filename + '.json', 'r') as f:\n",
    "        data_loaded = json.load(f)\n",
    "    return data_loaded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8d9cb5",
   "metadata": {},
   "source": [
    "## Pipeline Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71eb762e",
   "metadata": {},
   "source": [
    "<font size=\"3\"> \n",
    "Hyper-Parameter tuning\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a984a29",
   "metadata": {},
   "source": [
    "<font size=\"3\"> \n",
    "The following function on our machines took a long time to complete (36 hours) so we quote the result below.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592580af",
   "metadata": {},
   "outputs": [],
   "source": [
    "core_features = ['node_id', 'ETA_curr']\n",
    "calendar_features = ['cos_hour', 'sin_dayofweek', 'cos_dayofweek', 'sin_month', 'cos_month','sin_dayofmonth', 'cos_dayofmonth', 'sin_year', 'cos_year', 'sin_weekofyear', 'cos_weekofyear', 'sin_quarter_hour', 'cos_quarter_hour']\n",
    "rolling_avg_features = ['rolling_avg_4h', 'rolling_avg_12h', 'rolling_avg_68h', 'rolling_avg_476h', 'rolling_avg_20240h']\n",
    "lag_features = ['lag1h', 'lag4h', 'lag476h', 'lag20240h']\n",
    "\n",
    "feature_cols = core_features + calendar_features + rolling_avg_features + lag_features\n",
    "training_end_date = '2023-01-01'\n",
    "validation_end_date = '2024-01-01'\n",
    "test_end_date = '2025-01-10'\n",
    "\n",
    "\n",
    "x_train, y_train, x_eval, y_eval, original_classes = load_data(True, feature_cols, training_end_date, validation_end_date, test_end_date)\n",
    "best_params = parameter_tuning_validation(x_train, y_train, x_eval, y_eval, original_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c39b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_params= [\n",
    "#      {'criterion': 'entropy', 'max_depth': 16, 'min_samples_split': 8},\n",
    "#      {'n_estimators': 12, 'max_depth': 6, 'learning_rate': 0.5},\n",
    "#      {'alpha': 1},\n",
    "#      {'n_neighbors': 9, 'algorithm': 'kd_tree'},\n",
    "#      {'max_depth': 12, 'n_estimators': 16, 'min_samples_split': 8},\n",
    "#      {'learning_rate': 0.1, 'n_estimators': 16, 'max_depth': 4},\n",
    "#      {'learning_rate': 0.01, 'n_estimators': 40},\n",
    "#      {'hidden_layer_sizes': (50,50,50), 'activation': 'relu', 'solver': 'sgd', 'learning_rate': 'constant'}\n",
    "# ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822d562e",
   "metadata": {},
   "source": [
    "<font size=\"3\"> \n",
    "Training and evaluation using testset\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03b13f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test, original_classes = load_data(False, feature_cols, training_end_date, validation_end_date, test_end_date)\n",
    "results =  evaluate_models(x_train, y_train, x_test, y_test, original_classes, best_params)\n",
    "save_dict(metrics_path, 'ml_algo_test_metrics', results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150be8c3",
   "metadata": {},
   "source": [
    "<font size=\"3\"> \n",
    "Plot results for comparison purposes \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4de1916-c671-4032-be9a-2bb599110359",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_path=load_dict(metrics_path, 'ml_algo_test_metrics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7476c85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_metrics_train_val(a, plots_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a612b4d-237e-44a3-9694-92ddd0c8ef54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (bus_timetable_opt)",
   "language": "python",
   "name": "bus_timetable_opt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
