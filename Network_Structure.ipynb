{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "649ee991-4f6b-45a2-9519-193a001cadb7",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e1fedef-38cf-4f84-b426-98bec7cbabd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import osmnx as ox\n",
    "import networkx as nx\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from os.path import join\n",
    "from datetime import timedelta\n",
    "from itertools import product\n",
    "import random\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from scipy.spatial import cKDTree\n",
    "from scipy.special import factorial\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8e515a-66c5-4a84-bb7b-c3a8d2068429",
   "metadata": {},
   "source": [
    "## Load Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "797ec4c8-9b15-41f0-a71a-266688a88b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = join(\"io\", \"input\")\n",
    "output_path = join(\"io\", \"output\")\n",
    "experiments_path = join(\"io\", \"experiments\")\n",
    "plots_path = join(experiments_path, \"plots\")\n",
    "traffic_data_raw_path = join(input_path, \"traffic_data\")\n",
    "coordinates_columns = ['id', 'latitude', 'longitude']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5b2625d-65e8-4a32-ae9e-a2f88334a23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(join(input_path, \"base_bus_network.json\"), 'r', encoding='utf-8') as file:\n",
    "    bus_network = json.load(file)\n",
    "\n",
    "stops_df = pd.read_csv(join(input_path, \"base_stops.csv\"), encoding='utf-8', sep=';')\n",
    "traffic_edges_mapping_df = pd.read_csv(join(input_path, \"base_network_edges_imet_map.csv\"), encoding='utf-8', sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8ae669-3f1b-49ac-a1df-c956d584bd23",
   "metadata": {},
   "source": [
    "## Create Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b2774e2-a131-45f5-8499-943e9d6944ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_edges(bus_network, stops_df):\n",
    "\n",
    "    # Create an empty DataFrame for edges\n",
    "    edges = []\n",
    "\n",
    "    # Parse dictionary to create edges\n",
    "    for bus, stops in bus_network.items():\n",
    "        for i in range(len(stops)-1):\n",
    "            start_code, start_name = stops[i].split('-', 1)\n",
    "            end_code, end_name = stops[i+1].split('-', 1)\n",
    "            edges.append({\n",
    "                'start_node_name': start_name,\n",
    "                'end_node_name': end_name,\n",
    "                'start_node_id': start_code,\n",
    "                'end_node_id': end_code,\n",
    "                'edge_id': f'{start_code}_{end_code}',\n",
    "                'bus': [bus]\n",
    "            })\n",
    "    \n",
    "    # Convert list to DataFrame\n",
    "    edges_df = pd.DataFrame(edges)\n",
    "    \n",
    "    edges_df['start_node_id'] = edges_df['start_node_id'].astype(str)\n",
    "    edges_df['end_node_id'] = edges_df['end_node_id'].astype(str)\n",
    "    \n",
    "    stops_df['id'] = stops_df['id'].astype(str)\n",
    "    \n",
    "    # If an edge appears in multiple buses, concatenate bus names\n",
    "    edges_df = edges_df.groupby(['start_node_id', 'end_node_id', 'start_node_name', 'end_node_name', 'edge_id']).agg({\n",
    "        'bus': lambda x: list(set([b for sublist in x for b in sublist]))}).reset_index()\n",
    "    \n",
    "    edges_df = edges_df.merge(stops_df, how='left', left_on='start_node_id', right_on='id')\n",
    "    edges_df.rename(columns={'latitude': 'start_lat', 'longitude': 'start_long'}, inplace=True)\n",
    "    edges_df.drop(columns='id', inplace=True)\n",
    "    \n",
    "    edges_df = edges_df.merge(stops_df, how='left', left_on='end_node_id', right_on='id')\n",
    "    edges_df.rename(columns={'latitude': 'end_lat', 'longitude': 'end_long'}, inplace=True)\n",
    "    edges_df.drop(columns='id', inplace=True)\n",
    "    edges_mapped = pd.merge(edges_df, traffic_edges_mapping_df[['edge_id', 'edge_id_traffic']], on='edge_id', how='inner')\n",
    "    \n",
    "    return edges_mapped\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8814fef-cd4b-49ab-9f36-e042a6b3a45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find the nearest nodes on the graph\n",
    "def get_nearest_node(lat, lon, graph):\n",
    "    return ox.distance.nearest_nodes(graph, lon, lat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b93af999-b6e1-4871-b172-350449f4506a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distances(edges_df):\n",
    "    # Calculate the driving distances for each pair of start and end nodes in edges_df\n",
    "    \n",
    "    # Define the place and download the road network graph for driving\n",
    "    place_name = \"Thessaloniki, Greece\"\n",
    "    G = ox.graph_from_place(place_name, network_type='drive')\n",
    "\n",
    "    driving_distances = []  # List to store the distances\n",
    "    for index, row in edges_df.iterrows():\n",
    "        start_lat = row['start_lat']\n",
    "        start_lon = row['start_long']\n",
    "        end_lat = row['end_lat']\n",
    "        end_lon = row['end_long']\n",
    "\n",
    "        # Get the nearest network nodes to these coordinates\n",
    "        start_node = get_nearest_node(start_lat, start_lon, G)\n",
    "        end_node = get_nearest_node(end_lat, end_lon, G)\n",
    "\n",
    "        # Compute the shortest path based on the length of the edges\n",
    "        route = nx.shortest_path(G, start_node, end_node, weight='length')\n",
    "\n",
    "        # Calculate the total distance of the route in meters\n",
    "        total_distance = 0\n",
    "        for i in range(len(route) - 1):\n",
    "            edge_data = G.get_edge_data(route[i], route[i + 1])[0]  # Assumes there's at least one edge between nodes\n",
    "            total_distance += edge_data['length']\n",
    "\n",
    "        driving_distances.append(total_distance)\n",
    "\n",
    "    # Assign the computed distances to a new column in the DataFrame\n",
    "    edges_df['driving_distance'] = driving_distances\n",
    "\n",
    "    return edges_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15613c3e-35c0-4309-b76f-19b8ef2dad2c",
   "metadata": {},
   "source": [
    "## Preprocess Traffic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "635caef6-f43e-478e-adc7-21356b94acf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_csv(file_path, valid_ids, chunksize=1000):  # Adjust chunksize based on your needs\n",
    "    chunks = []\n",
    "    for chunk in pd.read_csv(file_path, chunksize=chunksize):\n",
    "        filtered_chunk = chunk[chunk['Link_ID'].isin(valid_ids)]\n",
    "        chunks.append(filtered_chunk)\n",
    "    return pd.concat(chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4384d40-d507-46a6-9534-02f4676ed9d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def merge_traffic_data(edges_df, traffic_data_raw_path, output_path, output_name = \"interm_traffic_data_all_merged.csv\"):\n",
    "    # Gather all valid 'edge_id_traffic' as a filter\n",
    "    valid_ids = set(edges_df['edge_id_traffic'])\n",
    "\n",
    "    output_file_path = join(output_path, output_name)\n",
    "    \n",
    "    if os.path.exists(output_file_path):\n",
    "        os.remove(output_file_path)\n",
    "    \n",
    "    # Directory containing all CSV files\n",
    "    header_written = False  # To ensure the header is written only once\n",
    "    \n",
    "    # Process each CSV file\n",
    "    for file_name in tqdm(os.listdir(traffic_data_raw_path)):\n",
    "        if file_name.endswith('.csv'):\n",
    "            print(file_name)\n",
    "            file_path = os.path.join(traffic_data_raw_path, file_name)\n",
    "            filtered_df = process_csv(file_path, valid_ids)\n",
    "    \n",
    "            if not header_written:\n",
    "                filtered_df.to_csv(output_file_path, mode='w', index=False)  # Write new file with header\n",
    "                header_written = True\n",
    "            else:\n",
    "                filtered_df.to_csv(output_file_path, mode='a', index=False, header=False)\n",
    "    \n",
    "    return header_written\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a10e07f-0590-49e6-8670-a9ff274928c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def refactor_traffic_data(edges_df, traffic_data_all_merged, subset, subset_start_date):\n",
    "    edges_columns = ['start_node_id', 'end_node_id', 'edge_id', 'edge_id_traffic', 'driving_distance']\n",
    "    traffic_data_columns = ['Link_ID', 'Timestamp', 'Avg_speed']\n",
    "    final_columns = ['node_id', 'timestamp', 'ETA']\n",
    "    \n",
    "    edges_df = edges_df[edges_columns]\n",
    "    traffic_data_all_merged = traffic_data_all_merged[traffic_data_columns].copy()\n",
    "    traffic_data_all_merged.rename(columns={'Link_ID': 'edge_id_traffic', 'Timestamp': 'timestamp', 'Avg_speed': 'avg_speed'}, inplace=True)\n",
    "    \n",
    "    \n",
    "    traffic_data_all_merged = pd.merge(edges_df, traffic_data_all_merged, on='edge_id_traffic', how='inner')\n",
    "    traffic_data_all_merged.rename(columns={'edge_id': 'node_id'}, inplace=True)\n",
    "\n",
    "    traffic_data_all_merged['ETA'] = traffic_data_all_merged['avg_speed']\n",
    "    traffic_data_all_merged = traffic_data_all_merged[final_columns]\n",
    "    \n",
    "    \n",
    "    traffic_data_all_merged['timestamp'] = traffic_data_all_merged['timestamp'].apply(\n",
    "        lambda x: x + \" 00:00:00\" if \" \" not in x else x\n",
    "    )\n",
    "    traffic_data_all_merged['timestamp'] = pd.to_datetime(traffic_data_all_merged['timestamp'])\n",
    "    \n",
    "    if subset:\n",
    "        traffic_data_all_merged = traffic_data_all_merged[traffic_data_all_merged['timestamp'] > subset_start_date]\n",
    "    \n",
    "    traffic_data_all_merged.to_csv(join(output_path, \"stg_traffic_data_all_merged.csv\"), sep=',', encoding='utf-8', index=False)\n",
    "\n",
    "    return traffic_data_all_merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "570657f3-07c6-4b71-b4e3-7eb8b222f05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fulfill_missing_targets(df):\n",
    "    # Extract the hour from the timestamp\n",
    "    df['hour'] = df['timestamp'].dt.hour\n",
    "    \n",
    "    # Calculate the mean 'ETA' for each 'node_id' for each hour across all dates\n",
    "    hourly_means = df.groupby(['node_id', 'hour'])['ETA'].mean().reset_index()\n",
    "    hourly_means.rename(columns={'ETA': 'hourly_ETA'}, inplace=True)\n",
    "    \n",
    "    # Generate all node-timestamp combinations\n",
    "    all_node_ids = df['node_id'].unique()\n",
    "    all_hours = df['hour'].unique()\n",
    "    \n",
    "    # Add full timestamp range to all_combinations for complete timeseries\n",
    "    min_date = df['timestamp'].min().normalize()  # Normalize to remove time part\n",
    "    max_date = df['timestamp'].max().normalize()\n",
    "    all_times = pd.date_range(start=min_date, end=max_date + timedelta(days=1), freq='15min')\n",
    "    all_times = all_times[(all_times.hour >= 7) & (all_times.hour <= 23)]\n",
    "    \n",
    "    \n",
    "    all_combinations = pd.DataFrame(list(product(all_node_ids, all_times)), columns=['node_id', 'timestamp'])\n",
    "    all_combinations['hour'] = all_combinations['timestamp'].dt.hour\n",
    "    \n",
    "    # Merge the full combinations with the original data\n",
    "    full_df = pd.merge(all_combinations, df.drop(columns='hour'), on=['node_id', 'timestamp'], how='left')\n",
    "    missing_data = full_df[full_df['ETA'].isnull()][['node_id', 'timestamp']]\n",
    "\n",
    "    full_df.sort_values(by=['node_id', 'timestamp'], inplace=True)\n",
    "    full_df['ETA'] = full_df.groupby('node_id')['ETA'].ffill().bfill()\n",
    "    \n",
    "    # Merge hourly mean speeds into the full DataFrame\n",
    "    full_df = pd.merge(full_df, hourly_means, on=['node_id', 'hour'], how='left')\n",
    "    \n",
    "    # Fill in missing 'ETA' values using the hourly mean 'ETA'\n",
    "    full_df['ETA'] = full_df['ETA'].fillna(full_df['hourly_ETA'])\n",
    "    full_df.drop(columns=['hourly_ETA'], inplace=True)\n",
    "\n",
    "    #full_df.to_csv(join(output_path, \"fct_traffic_data_all_merged.csv\"), sep=',', encoding='utf-8', index=False)\n",
    "    #missing_data.to_csv(join(output_path, \"base_missing_raw_targets.csv\"), sep=',', encoding='utf-8', index=False)\n",
    "    \n",
    "    return full_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b20f06-00e8-4422-a690-5eb180caa5b3",
   "metadata": {},
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04b0eb66-7fc3-4152-9811-d004c3e84b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_start_date = '2024-10-31'\n",
    "subset = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37868c2f-c456-4f41-860a-be85e5f9d746",
   "metadata": {},
   "source": [
    "## Intermidiate Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6516b89d-77cd-474a-89f7-8f52b49a37ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_df = create_edges(bus_network, stops_df[coordinates_columns].copy())\n",
    "edges_df = calculate_distances(edges_df)\n",
    "edges_df.to_csv(join(output_path, \"interm_network_edges.csv\"), sep=',', encoding='utf-8', index=False)\n",
    "status =  merge_traffic_data(edges_df, traffic_data_raw_path, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f3523d-5bc7-48a5-8a0f-f1526667cd3b",
   "metadata": {},
   "source": [
    "## Data Cleaning & Missing Values Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b47c65b9-52f5-481d-a135-dc29369e860c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>node_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>hour</th>\n",
       "      <th>ETA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1004_1256</td>\n",
       "      <td>2022-01-01 07:00:00</td>\n",
       "      <td>7</td>\n",
       "      <td>41.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1004_1256</td>\n",
       "      <td>2022-01-01 07:15:00</td>\n",
       "      <td>7</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1004_1256</td>\n",
       "      <td>2022-01-01 07:30:00</td>\n",
       "      <td>7</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1004_1256</td>\n",
       "      <td>2022-01-01 07:45:00</td>\n",
       "      <td>7</td>\n",
       "      <td>56.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1004_1256</td>\n",
       "      <td>2022-01-01 08:00:00</td>\n",
       "      <td>8</td>\n",
       "      <td>56.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16937095</th>\n",
       "      <td>45129_45113</td>\n",
       "      <td>2025-01-11 22:45:00</td>\n",
       "      <td>22</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16937096</th>\n",
       "      <td>45129_45113</td>\n",
       "      <td>2025-01-11 23:00:00</td>\n",
       "      <td>23</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16937097</th>\n",
       "      <td>45129_45113</td>\n",
       "      <td>2025-01-11 23:15:00</td>\n",
       "      <td>23</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16937098</th>\n",
       "      <td>45129_45113</td>\n",
       "      <td>2025-01-11 23:30:00</td>\n",
       "      <td>23</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16937099</th>\n",
       "      <td>45129_45113</td>\n",
       "      <td>2025-01-11 23:45:00</td>\n",
       "      <td>23</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16937100 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              node_id           timestamp  hour   ETA\n",
       "0           1004_1256 2022-01-01 07:00:00     7  41.0\n",
       "1           1004_1256 2022-01-01 07:15:00     7  36.0\n",
       "2           1004_1256 2022-01-01 07:30:00     7  45.0\n",
       "3           1004_1256 2022-01-01 07:45:00     7  56.0\n",
       "4           1004_1256 2022-01-01 08:00:00     8  56.0\n",
       "...               ...                 ...   ...   ...\n",
       "16937095  45129_45113 2025-01-11 22:45:00    22  45.0\n",
       "16937096  45129_45113 2025-01-11 23:00:00    23  45.0\n",
       "16937097  45129_45113 2025-01-11 23:15:00    23  45.0\n",
       "16937098  45129_45113 2025-01-11 23:30:00    23  45.0\n",
       "16937099  45129_45113 2025-01-11 23:45:00    23  45.0\n",
       "\n",
       "[16937100 rows x 4 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edges_df = pd.read_csv(join(output_path, \"interm_network_edges.csv\"), encoding='utf-8', sep=',')\n",
    "traffic_data_all_merged = pd.read_csv(join(output_path, \"interm_traffic_data_all_merged.csv\"), encoding='utf-8', sep=',')\n",
    "traffic_data_all_merged = traffic_data_all_merged.drop_duplicates()\n",
    "traffic_data_all_merged.to_csv(join(output_path, \"interm_traffic_data_all_merged.csv\"), sep=',', encoding='utf-8', index=False)\n",
    "\n",
    "final_traffic_data_all_merged = refactor_traffic_data(edges_df, traffic_data_all_merged, subset, subset_start_date)\n",
    "final_traffic_data_all_merged = fulfill_missing_targets(final_traffic_data_all_merged)\n",
    "final_traffic_data_all_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae03530e-d505-41b5-a08c-1e8451219e3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (bus_timetable_opt)",
   "language": "python",
   "name": "bus_timetable_opt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
