{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76a74cfc-5864-40f4-ae4f-deab70bbee1b",
   "metadata": {},
   "source": [
    "## Libraries - Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bfaca77d-944b-437a-8ef6-4b61f584ff23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from os.path import join\n",
    "from data_handler import FeatureEngineering\n",
    "import gc\n",
    "from scipy.stats import norm\n",
    "from scipy.signal import convolve\n",
    "import ast\n",
    "import copy\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40fb04d-ad6c-4baf-bfee-ac303406fc9d",
   "metadata": {},
   "source": [
    "## Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f05c77d4-79cf-4672-a8c4-ac4aaa08f4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = join(\"io\", \"input\")\n",
    "output_path = join(\"io\", \"output\")\n",
    "experiments_path = join(\"io\", \"experiments\")\n",
    "graph_structured_np = join(output_path, \"graph_structured_np\")\n",
    "metrics_path = join(experiments_path, \"metrics\")\n",
    "plots_path = join(experiments_path, \"plots\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3087ed-8fe2-4032-9684-62e5f8c81219",
   "metadata": {},
   "source": [
    "## Preprocess ETA Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f20e77b7-4fa1-451f-aa75-211d2ff821c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_eta_distributions(avg_speed_forecasts_df, edges_df):\n",
    "    merged_df = pd.merge(avg_speed_forecasts_df, edges_df, on='edge_id', how='left')\n",
    "    \n",
    "    # Convert speed mean from km/h to meters/minute (1 km/h = 1000 m/60 min)\n",
    "    merged_df['speed_m_per_min'] = merged_df['mean'] * 1000 / 60\n",
    "    # Calculate mean ETA in minutes\n",
    "    merged_df['mean_eta'] = merged_df['driving_distance'] * 2 / merged_df['speed_m_per_min']\n",
    "\n",
    "    # Convert variance of ETA to log variance for consistency with other columns\n",
    "    merged_df['log_var_eta'] = merged_df['log_var'] / 2\n",
    "    # Create the final DataFrame with selected columns\n",
    "    eta_distribution_df = merged_df[['edge_id', 'timestamp', 'mean_eta', 'log_var_eta']]\n",
    "    return eta_distribution_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b81fbc3b-48be-4d0a-96a3-f2c88c20855b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_probabilities(row, minutes_range):\n",
    "    mean_eta = row['mean_eta']\n",
    "    std_eta = row['std_eta']\n",
    "    # Calculate probabilities for each minute\n",
    "    probabilities = [norm.cdf(minute, mean_eta, std_eta) - norm.cdf(minute - 1, mean_eta, std_eta) for minute in minutes_range]\n",
    "    \n",
    "    # Threshold probabilities below 0.001 to zero\n",
    "    probabilities = [p if p >= 0.01 else 0 for p in probabilities]\n",
    "    \n",
    "    # Renormalize probabilities to sum to 1 if not all are zero\n",
    "    total_prob = sum(probabilities)\n",
    "    if total_prob > 0:\n",
    "        probabilities = [p / total_prob for p in probabilities]\n",
    "\n",
    "    probabilities.insert(0, 0)\n",
    "    \n",
    "    # Trim trailing zeros from the list\n",
    "    while probabilities and probabilities[-1] == 0:\n",
    "        probabilities.pop()\n",
    "    \n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0fe5a8d5-e219-453e-8ee9-4c12b7fc2bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conver_distributions_tolist(eta_distribution_df):\n",
    "\n",
    "    # Calculate standard deviation from log variance\n",
    "    eta_distribution_df['std_eta'] = np.sqrt(np.exp(eta_distribution_df['log_var_eta']))\n",
    "    \n",
    "    # Find the maximum standard deviation to determine the needed range\n",
    "    max_std_eta = eta_distribution_df['std_eta'].max()\n",
    "    \n",
    "    # Define the maximum range of ETA values based on the maximum standard deviation\n",
    "    # Assuming we consider mean plus/minus 3 standard deviations to cover 99.7% of the data\n",
    "    max_eta = int(np.ceil(eta_distribution_df['mean_eta'].max() + 3 * max_std_eta))\n",
    "    min_eta = max(1, int(np.floor(eta_distribution_df['mean_eta'].min() - 3 * max_std_eta)))\n",
    "    \n",
    "    # Minutes range based on the dataset's maximum variability\n",
    "    minutes_range = np.arange(min_eta, max_eta + 1)\n",
    "\n",
    "    eta_distribution_df['prob_distributions'] = eta_distribution_df.apply(calculate_probabilities, axis=1, args=(minutes_range,))\n",
    "\n",
    "    return eta_distribution_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bfbf06a0-b0b2-43d4-8eab-0aaa1ccaedf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eta_distributions_creation(avg_speed_forecasts_df, edges_df):\n",
    "\n",
    "    avg_speed_forecasts_df.rename(columns={'node_id': 'edge_id'}, inplace=True)\n",
    "    \n",
    "    eta_distribution_df = calculate_eta_distributions(avg_speed_forecasts_df, edges_df)\n",
    "    eta_distribution_df = conver_distributions_tolist(eta_distribution_df)\n",
    "    eta_distribution_df.to_csv(join(output_path, \"interm_eta_distributions.csv\"), sep=',', encoding='utf-8', index=False)\n",
    "\n",
    "    eta_distribution_df['minute_of_day'] = pd.to_datetime(eta_distribution_df['timestamp']).dt.hour * 60 + pd.to_datetime(eta_distribution_df['timestamp']).dt.minute\n",
    "    eta_distribution_df['minute_of_schedule'] = eta_distribution_df['minute_of_day'] - 419\n",
    "    eta_distribution_df['ETA'] = (eta_distribution_df['mean_eta'].round() + 1).astype(int)\n",
    "    eta_distribution_df['prob_distributions'] = eta_distribution_df['prob_distributions'].apply(lambda x: [float(i) for i in x])\n",
    "\n",
    "    final_columns = ['edge_id','minute_of_day', 'minute_of_schedule', 'ETA', 'prob_distributions']\n",
    "    eta_distribution_df = eta_distribution_df[final_columns]\n",
    "    eta_distribution_df.to_csv(join(output_path, \"stg_eta_distributions.csv\"), sep=',', encoding='utf-8', index=False)\n",
    "\n",
    "    return eta_distribution_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8f6671b4-262c-47e9-a35e-2cf620550efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normilize_probabilistic_distribution(ls, threshold=0.01):\n",
    "    \"\"\"\n",
    "    Process a probabilistic distribution by:\n",
    "    1. Setting all probabilities ≤ threshold to zero.\n",
    "    2. Renormalizing the probabilities to sum to 1, unless all are below threshold (then keep original).\n",
    "    3. Removing trailing zeros at the end of the list.\n",
    "    \n",
    "    Parameters:\n",
    "    - ls (list): The input probabilistic distribution.\n",
    "    - threshold (float): The minimum probability value to keep (default is 1%).\n",
    "\n",
    "    Returns:\n",
    "    - list: The processed probabilistic distribution.\n",
    "    \"\"\"\n",
    "    # Convert to numpy array for efficient computation\n",
    "    ls = np.array(ls, dtype=np.float64)\n",
    "    \n",
    "    # Step 1: Set probabilities ≤ threshold to zero\n",
    "    filtered_ls = np.where(ls <= threshold, 0, ls)  # Replace values ≤ threshold with 0\n",
    "\n",
    "    # Step 2: Check if all probabilities were below threshold before filtering\n",
    "    if np.sum(filtered_ls) > 0:  # Only normalize if there are values left\n",
    "        filtered_ls = filtered_ls / np.sum(filtered_ls)\n",
    "    else:\n",
    "        filtered_ls = ls  # Keep original if all values were below threshold\n",
    "\n",
    "    # Step 3: Remove trailing zeros\n",
    "    filtered_ls = np.trim_zeros(filtered_ls, trim='b')  # 'b' means trim from the end only\n",
    "    \n",
    "    return filtered_ls.tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cafadd2-17d1-4299-942b-b72ad28cbf8e",
   "metadata": {},
   "source": [
    "## Create Possbile Bus Schedules Start Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f2262048-1d99-431b-b0ac-bf3a41cc2976",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bus_schedules(network, start_min, end_min, interval, stochastic=False):\n",
    "    schedules = {}\n",
    "    \n",
    "    # Iterate over each route in the network dictionary\n",
    "    for route, stops in network.items():\n",
    "        if stops:  # Ensure there are stops in the list\n",
    "            first_stop = stops[0]\n",
    "            # Generate start times from start_min to end_min every 'interval' minutes\n",
    "            start_times = list(range(start_min, end_min + 1, interval))\n",
    "            \n",
    "            if not stochastic:\n",
    "                # Deterministic mode: Return the start times as a simple list\n",
    "                schedules[route] = {first_stop: start_times}\n",
    "            else:\n",
    "                # Stochastic mode: Convert start times into trimmed probabilistic lists\n",
    "                prob_distributions = []\n",
    "                for start_time in start_times:\n",
    "                    prob_list = [0] * start_time  # Create a list only up to the start time\n",
    "                    prob_list[start_time - 1] = 1  # Set 100% probability at the specific start time\n",
    "                    prob_distributions.append(prob_list)\n",
    "                \n",
    "                schedules[route] = {first_stop: prob_distributions}\n",
    "\n",
    "    return schedules\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e83994d-6005-45c6-99e2-0a17320b55e6",
   "metadata": {},
   "source": [
    "## Calculate Buses Arival Time Using ETA Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8c4115e1-5c43-4918-a2b0-90854bc8ddad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_probabilistic_bus_schedules(bus_network, bus_schedules, eta_distribution_df):\n",
    "    # Convert eta_distribution_df to a dictionary for fast lookup\n",
    "    \n",
    "    if isinstance(eta_distribution_df['prob_distributions'].iloc[0], str):\n",
    "        eta_distribution_df['prob_distributions'] = eta_distribution_df['prob_distributions'].apply(ast.literal_eval)\n",
    "\n",
    "    eta_dict = {(row['edge_id'], row['minute_of_schedule']): row['prob_distributions'] for _, row in eta_distribution_df.iterrows()}\n",
    "    \n",
    "    schedules = {}  # Final output structure\n",
    "\n",
    "    for bus, schedules_dict in tqdm(bus_schedules.items()):\n",
    "        # Initialize the nested dictionary for each bus\n",
    "        schedules[bus] = {}\n",
    "        \n",
    "        # Get the route stops for this bus\n",
    "        route_stops = bus_network.get(bus, [])\n",
    "        \n",
    "        # Ensure we have a valid route\n",
    "        if not route_stops:\n",
    "            continue  \n",
    "\n",
    "        # Extract the first stop\n",
    "        first_stop = route_stops[0]\n",
    "\n",
    "        # Get the list of start time distributions for this bus route\n",
    "        start_time_distributions = schedules_dict.get(first_stop, [])  # Extract list of probability distributions\n",
    "        \n",
    "        for start_time_distribution in start_time_distributions:  # Iterate over each schedule (as a probability list)\n",
    "            # Initialize arrival distributions for this schedule\n",
    "            arrival_distributions = {first_stop: start_time_distribution}  # Start stop follows the given probability distribution\n",
    "            \n",
    "            # Iterate over the stops in sequence\n",
    "            for i in range(len(route_stops) - 1):\n",
    "                current_stop = route_stops[i]\n",
    "                next_stop = route_stops[i + 1]\n",
    "                edge_id = f\"{current_stop}_{next_stop}\"\n",
    "\n",
    "                # Find the closest ETA entry in the dataset\n",
    "                current_distribution = arrival_distributions[current_stop]  # Arrival distribution at current stop\n",
    "                \n",
    "                # Find the closest available time in eta_distribution_df\n",
    "                available_times = [minute for (edge, minute) in eta_dict.keys() if edge == edge_id]\n",
    "                \n",
    "                if not available_times:\n",
    "                    continue  # Skip if there are no ETA values for this route\n",
    "                \n",
    "                # Find the closest minute available\n",
    "                closest_minute = min(available_times, key=lambda x: abs(x - np.argmax(current_distribution)))  \n",
    "                eta_distribution = eta_dict.get((edge_id, closest_minute), [0])  # Default to a zero probability list if not found\n",
    "                \n",
    "                # Compute arrival distribution at the next stop using convolution\n",
    "                arrival_distribution_next_stop = np.convolve(current_distribution, eta_distribution, mode='full')  \n",
    "                arrival_distribution_next_stop = normilize_probabilistic_distribution(arrival_distribution_next_stop)\n",
    "                # Store the computed arrival distribution\n",
    "                arrival_distributions[next_stop] = arrival_distribution_next_stop\n",
    "\n",
    "            # Store the computed schedule for this departure time\n",
    "            for stop, arrival_distribution in arrival_distributions.items():\n",
    "                if stop not in schedules[bus]:\n",
    "                    schedules[bus][stop] = []\n",
    "                schedules[bus][stop].append(arrival_distribution)\n",
    "\n",
    "    return schedules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "59233b98-313a-4a0b-8bc9-e261e936ad65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_key_to_front(schedules, foo_line):\n",
    "    \"\"\"\n",
    "    Reorders the schedules dict so that foo_line is the first key.\n",
    "    \"\"\"\n",
    "    if foo_line not in schedules:\n",
    "        return schedules  # nothing to reorder\n",
    "\n",
    "    # Reconstruct dict with 'b0_w' first\n",
    "    reordered = {foo_line: schedules[foo_line]}\n",
    "    for k, v in schedules.items():\n",
    "        if k != foo_line:\n",
    "            reordered[k] = v\n",
    "    return reordered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8c7a7475-397a-47dd-80a9-3280efc5f834",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_schedule_limits(schedules, stops, start, end, foo_line='b00_w'):\n",
    "    \"\"\"\n",
    "    Adds a hardcoded bus line 'b0_w' to the schedules dictionary for each stop,\n",
    "    inserting two distributions:\n",
    "    - One as a single value list: [start]\n",
    "    - One as a probability distribution list where index `end-1` has value 1\n",
    "    \"\"\"\n",
    "    if foo_line not in schedules:\n",
    "        schedules[foo_line] = {}\n",
    "\n",
    "    for stop in stops:\n",
    "        # Create the two probability-style distributions\n",
    "        first = [start]\n",
    "        second = [0] * end\n",
    "        second[end - 1] = 1  # Set 1 at the end-1 index (since list is 0-indexed)\n",
    "\n",
    "        schedules[foo_line][stop] = [first, second]\n",
    "\n",
    "    schedules = move_key_to_front(schedules, foo_line)\n",
    "    \n",
    "    return schedules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b2b4a010-b26e-480b-aa77-ef08cd0359e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_distributions_to_means(schedules):\n",
    "    schedules_mean = copy.deepcopy(schedules)\n",
    "    \n",
    "    for bus in schedules_mean:\n",
    "        for stop in schedules_mean[bus]:\n",
    "            # Compute the mean for each distribution and flatten the result\n",
    "            mean_list = []\n",
    "            for prob_dist in schedules_mean[bus][stop]:\n",
    "                prob_dist = np.array(prob_dist, dtype=np.float64)\n",
    "                mean = np.sum(prob_dist * np.arange(1, len(prob_dist)+1))\n",
    "                mean_list.append(round(mean))\n",
    "            schedules_mean[bus][stop] = mean_list\n",
    "\n",
    "    return schedules_mean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ebfe9a-791d-4213-a20d-622910561713",
   "metadata": {},
   "source": [
    "## Preprocess Passengers Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "aff13ba4-62ed-4a2c-bd2c-7d71af33e675",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_passengers_demand(passengers_demand):\n",
    "    passengers_demand['minute_of_day'] = pd.to_datetime(passengers_demand['timestamp']).dt.hour * 60 + pd.to_datetime(passengers_demand['timestamp']).dt.minute\n",
    "    passengers_demand['minute_of_schedule'] = passengers_demand['minute_of_day'] - 419\n",
    "    return passengers_demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5e18d1ac-338a-45f8-9608-f4a61a9ffb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_passenger_arrivals_to_dict(df):\n",
    "    \"\"\"\n",
    "    Convert a DataFrame with stop_id, passenger_demand_distribution, and minute_of_schedule\n",
    "    into a dictionary format where stop_id is the key, and the value is a list of passenger demand distributions\n",
    "    indexed by minute_of_schedule.\n",
    "    \"\"\"\n",
    "    passenger_arrivals = {}\n",
    "\n",
    "    if isinstance(df['passenger_demand_distribution'].iloc[0], str):\n",
    "        df['passenger_demand_distribution'] = df['passenger_demand_distribution'].apply(ast.literal_eval)\n",
    "\n",
    "    for _, row in tqdm(df.iterrows()):\n",
    "        stop = str(row[\"stop_id\"])\n",
    "        minute = row[\"minute_of_schedule\"]\n",
    "        demand_distribution = row[\"passenger_demand_distribution\"]\n",
    "        \n",
    "        if stop not in passenger_arrivals:\n",
    "            passenger_arrivals[stop] = []\n",
    "\n",
    "        # Ensure correct indexing by padding empty lists if necessary\n",
    "        while len(passenger_arrivals[stop]) < minute:\n",
    "            passenger_arrivals[stop].append([])  # Fill missing time slots with empty lists\n",
    "        \n",
    "        # Insert the distribution at the correct minute index (adjust for 0-based indexing)\n",
    "        passenger_arrivals[stop][minute - 1] = demand_distribution  \n",
    "\n",
    "    return passenger_arrivals\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92e0372-4509-4a6f-a65b-9a42f887ae2a",
   "metadata": {},
   "source": [
    "## Calculate Stochastic Gap-Based Passenger Demand "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9f091565-0c76-4bf4-bf2c-8c7c6568f665",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_probabilistic_gaps(stops, schedules, schedules_mean, passenger_arrivals):\n",
    "    gap_passengers_dict = {}\n",
    "\n",
    "    for stop in tqdm(stops):  # Loop over each stop\n",
    "        if stop not in passenger_arrivals:\n",
    "            continue  # Skip if no passenger data for this stop\n",
    "\n",
    "        all_eta_distributions = []  # Store all ETA distributions at this stop\n",
    "        mean_eta_values = []  # Store mean ETA values at this stop\n",
    "\n",
    "        # Gather all possible ETA distributions for this stop from all bus lines\n",
    "        for bus in schedules:\n",
    "            if stop in schedules[bus]:  # Check if this bus stops here\n",
    "                for eta_dist in schedules[bus][stop]:\n",
    "                    all_eta_distributions.append(np.array(eta_dist, dtype=np.float64))  # Ensure float type\n",
    "                for mean_eta in schedules_mean[bus][stop]:\n",
    "                    mean_eta_values.append(mean_eta)  # Extract the integer mean ETA value\n",
    "\n",
    "        # Compute gaps between all possible arrival pairs\n",
    "        for i in range(len(all_eta_distributions) - 1):\n",
    "            for j in range(i + 1, len(all_eta_distributions)):\n",
    "                eta_1 = all_eta_distributions[i]\n",
    "                eta_2 = all_eta_distributions[j]\n",
    "                mean_eta_1 = mean_eta_values[i]\n",
    "                mean_eta_2 = mean_eta_values[j]\n",
    "\n",
    "                # Determine which ETA has the higher mean\n",
    "                if mean_eta_1 > mean_eta_2:\n",
    "                    max_eta, min_eta = eta_1, eta_2\n",
    "                    max_mean, min_mean = mean_eta_1, mean_eta_2\n",
    "                elif mean_eta_2 > mean_eta_1:\n",
    "                    max_eta, min_eta = eta_2, eta_1\n",
    "                    max_mean, min_mean = mean_eta_2, mean_eta_1\n",
    "                else:\n",
    "                    gap_passengers_dict[f\"gap_{stop}_{i}_{j}_{mean_eta_2}_{mean_eta_1}\"] = 0\n",
    "                    continue  # Skip calculation if means are equal (gap = 0)\n",
    "\n",
    "                # Compute cumulative sums (CDFs) of both distributions\n",
    "                max_cdf = np.cumsum(max_eta)\n",
    "                min_cdf = np.cumsum(min_eta)\n",
    "\n",
    "                # Ensure both distributions have the same length by padding the smaller one with 1s\n",
    "                if len(min_cdf) < len(max_cdf):\n",
    "                    min_cdf = np.pad(min_cdf, (0, len(max_cdf) - len(min_cdf)), mode='constant', constant_values=1)\n",
    "                elif len(min_cdf) > len(max_cdf):\n",
    "                    max_cdf = np.pad(max_cdf, (0, len(min_cdf) - len(max_cdf)), mode='constant', constant_values=1)\n",
    "\n",
    "                # Compute probability mass between these distributions\n",
    "                prob_between_buses = np.maximum(min_cdf - max_cdf, 0)  # Ensure no negative values\n",
    "                # Retrieve the correct passenger arrival distributions for this stop\n",
    "                passengers_at_stop = passenger_arrivals[stop]\n",
    "\n",
    "                # Step 1: Multiply each `prob_between_buses` value with the passenger probability distribution at the corresponding timestep\n",
    "                multiplied_distributions = []\n",
    "                for t in range(len(prob_between_buses)):\n",
    "                    if prob_between_buses[t] == 0:\n",
    "                        continue\n",
    "                    if t < len(passengers_at_stop):\n",
    "                        passengers_dist = np.array(passengers_at_stop[t], dtype=np.float64)  # Convert to NumPy array\n",
    "                        multiplied_dist = passengers_dist * prob_between_buses[t]  # Element-wise multiplication\n",
    "                        multiplied_distributions.append(multiplied_dist)\n",
    "                \n",
    "                # Step 2: Convolve all multiplied distributions sequentially\n",
    "                if multiplied_distributions:\n",
    "                    convolved_distribution = multiplied_distributions[0]  # Start with the first distribution\n",
    "                    for k in range(1, len(multiplied_distributions)):\n",
    "                        convolved_distribution = np.convolve(convolved_distribution, multiplied_distributions[k], mode='full')\n",
    "                        total = convolved_distribution.sum()\n",
    "                        if total > 0:\n",
    "                            convolved_distribution = convolved_distribution / total\n",
    "\n",
    "                    # Step 3: Compute the expected number of remaining passengers (mean of final convolved distribution)\n",
    "                    expected_remaining_passengers = np.sum(convolved_distribution * np.arange(1,len(convolved_distribution)+1))\n",
    "                else:\n",
    "                    expected_remaining_passengers = 0  # Default if no valid distributions exist\n",
    "\n",
    "                # Store result in gap_passengers_dict\n",
    "                gap_key = f\"gap_{stop}_{i}_{j}_{mean_eta_1}_{mean_eta_2}\"\n",
    "                gap_passengers_dict[gap_key] = round(float(expected_remaining_passengers), 4)\n",
    "\n",
    "    return gap_passengers_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fe0c88-eb3e-44e7-8a1b-b5cfc31f182a",
   "metadata": {},
   "source": [
    "## Dictionary IO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ef298472-1f18-4aae-b6a9-047dfa1af355",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dict_to_file(dictionary, filepath):\n",
    "    \"\"\"\n",
    "    Save a dictionary to a file using pickle.\n",
    "    \"\"\"\n",
    "    with open(filepath, 'wb') as file:\n",
    "        pickle.dump(dictionary, file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1aa9b0f-1240-42a2-a3c0-695fd12270b0",
   "metadata": {},
   "source": [
    "# Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b415afe4-2265-42c4-ac84-18427975546f",
   "metadata": {},
   "source": [
    "## Schedule Paramters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6b98cdc5-6473-435d-8e65-a439dee1788b",
   "metadata": {},
   "outputs": [],
   "source": [
    "schedule_start = 1\n",
    "scedule_end = 120  #840\n",
    "bus_schedule_frequency = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fdd1ae-22a8-437c-a1ea-cfc3d73eeff7",
   "metadata": {},
   "source": [
    "## Load Appropriate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c865e113-9bec-4efe-9dc2-e98d5e96ea94",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_df = pd.read_csv(join(output_path, \"interm_network_edges.csv\"), encoding='utf-8', sep=',')\n",
    "avg_speed_forecasts_df = pd.read_csv(join(output_path, \"interm_avg_speed_forecasts.csv\"), encoding='utf-8', sep=',')\n",
    "passengers_demand_df = pd.read_csv(join(output_path, \"interm_passenger_demand.csv\"), encoding='utf-8', sep=',')\n",
    "\n",
    "with open(join(input_path, \"base_bus_network.json\"), 'r', encoding='utf-8') as file:\n",
    "    bus_network = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8281b014-fedd-42a2-b342-aa33925cc28a",
   "metadata": {},
   "source": [
    "## Create Probabilistic Schedules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e33438a8-f7bb-4fb1-8751-bca6effddb9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 12/12 [00:02<00:00,  4.36it/s]\n"
     ]
    }
   ],
   "source": [
    "eta_distribution_df = eta_distributions_creation(avg_speed_forecasts_df, edges_df)\n",
    "passengers_demand_df = preprocess_passengers_demand(passengers_demand_df)\n",
    "\n",
    "bus_network = {route: [stop.split('-')[0] for stop in stops] for route, stops in bus_network.items()}\n",
    "bus_schedules = create_bus_schedules(bus_network, schedule_start, scedule_end, bus_schedule_frequency, True)\n",
    "\n",
    "schedules = compute_probabilistic_bus_schedules(bus_network, bus_schedules, eta_distribution_df)\n",
    "schedules = add_schedule_limits(schedules, stops, schedule_start, scedule_end)\n",
    "schedules_mean = convert_distributions_to_means(schedules)\n",
    "save_dict_to_file(schedules_mean, join(output_path, \"fct_schedules_mean_sub\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d81a39-83c7-4335-a74a-ddf5da28dd91",
   "metadata": {},
   "source": [
    "## Prepare Probabilistic Passengers Demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f398954f-680a-4e00-bb70-f2c898224c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "211050it [00:06, 33685.29it/s]\n"
     ]
    }
   ],
   "source": [
    "passengers_demand_df = pd.read_csv(join(output_path, \"interm_passenger_demand.csv\"), encoding='utf-8', sep=',')\n",
    "\n",
    "passengers_demand_df = preprocess_passengers_demand(passengers_demand_df)\n",
    "passengers_demand = convert_passenger_arrivals_to_dict(passengers_demand_df)\n",
    "stops = sorted(list(passengers_demand.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e167ddc-6b89-468f-8db0-e0af03af9aa4",
   "metadata": {},
   "source": [
    "## Stochastic Gap-Based Passenger Demand "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "87ee366c-78da-4bb7-a674-375f4a49228f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 210/210 [00:50<00:00,  4.14it/s]\n"
     ]
    }
   ],
   "source": [
    "# Run the function\n",
    "gap_passengers_dict = compute_probabilistic_gaps(stops, schedules, schedules_mean, passengers_demand)\n",
    "save_dict_to_file(gap_passengers_dict, join(output_path, \"fct_gap_passengers_sub\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ce8e08-6fff-409b-865e-58781b0bce75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba355b5-87c2-4921-99ad-760fb73e8d65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ca04cd-74c1-4d7f-a8dc-f8c949d11169",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Torch (bus_sch))",
   "language": "python",
   "name": "torch_geometric"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
